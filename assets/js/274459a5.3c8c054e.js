"use strict";(self.webpackChunkmybooks=self.webpackChunkmybooks||[]).push([[8566],{6368:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>h});var r=t(4848),i=t(8453);const o={},a=void 0,s={id:"Craftinginterpreters/not-translated-yet/optimization",title:"optimization",description:"The evening's the best part of the day. You've done your day's work. Now you",source:"@site/docs/Craftinginterpreters/not-translated-yet/optimization.md",sourceDirName:"Craftinginterpreters/not-translated-yet",slug:"/Craftinginterpreters/not-translated-yet/optimization",permalink:"/docs/Craftinginterpreters/not-translated-yet/optimization",draft:!1,unlisted:!1,editUrl:"https://github.com/jabberwocky238/jabberwocky238.github.io/docs/Craftinginterpreters/not-translated-yet/optimization.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"methods-and-initializers",permalink:"/docs/Craftinginterpreters/not-translated-yet/methods-and-initializers"},next:{title:"parsing-expressions",permalink:"/docs/Craftinginterpreters/not-translated-yet/parsing-expressions"}},l={},h=[{value:"Measuring Performance",id:"measuring-performance",level:2},{value:"Benchmarks",id:"benchmarks",level:3},{value:"Profiling",id:"profiling",level:3},{value:"Faster Hash Table Probing",id:"faster-hash-table-probing",level:2},{value:"Slow key wrapping",id:"slow-key-wrapping",level:3},{value:"NaN Boxing",id:"nan-boxing",level:2},{value:"What is (and is not) a number?",id:"what-is-and-is-not-a-number",level:3},{value:"Conditional support",id:"conditional-support",level:3},{value:"Numbers",id:"numbers",level:3},{value:"Nil, true, and false",id:"nil-true-and-false",level:3},{value:"Objects",id:"objects",level:3},{value:"Value functions",id:"value-functions",level:3},{value:"Evaluating performance",id:"evaluating-performance",level:3},{value:"Where to Next",id:"where-to-next",level:2},{value:"Challenges",id:"challenges",level:2}];function c(e){const n={a:"a",aside:"aside",blockquote:"blockquote",cite:"cite",code:"code",div:"div",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"The evening's the best part of the day. You've done your day's work. Now you\r\ncan put your feet up and enjoy it."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.cite,{children:["Kazuo Ishiguro, ",(0,r.jsx)(n.em,{children:"The Remains of the Day"})]})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["If I still lived in New Orleans, I'd call this chapter a ",(0,r.jsx)(n.em,{children:"lagniappe"}),", a little\r\nsomething extra given for free to a customer. You've got a whole book and a\r\ncomplete virtual machine already, but I want you to have some more fun hacking\r\non clox. This time, we're going for pure performance. We'll apply two very\r\ndifferent optimizations to our virtual machine.  In the process, you'll get a\r\nfeel for measuring and improving the performance of a language implementation --\r\nor any program, really."]}),"\n",(0,r.jsx)(n.h2,{id:"measuring-performance",children:"Measuring Performance"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optimization"})," means taking a working application and improving its\r\nperformance. An optimized program does the same thing, it just takes less\r\nresources to do so. The resource we usually think of when optimizing is runtime\r\nspeed, but it can also be important to reduce memory usage, startup time,\r\npersistent storage size, or network bandwidth. All physical resources have some\r\ncost -- even if the cost is mostly in wasted human time -- so optimization work\r\noften pays off."]}),"\n",(0,r.jsx)(n.p,{children:'There was a time in the early days of computing that a skilled programmer could\r\nhold the entire hardware architecture and compiler pipeline in their head and\r\nunderstand a program\'s performance just by thinking real hard. Those days are\r\nlong gone, separated from the present by microcode, cache lines, branch\r\nprediction, deep compiler pipelines, and mammoth instruction sets. We like to\r\npretend C is a "low-level" language, but the stack of technology between'}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c",children:'printf("Hello, world!");\n'})}),"\n",(0,r.jsx)(n.p,{children:"and a greeting appearing on screen is now perilously tall."}),"\n",(0,r.jsxs)(n.p,{children:["Optimization today is an empirical science. Our program is a border collie\r\nsprinting through the hardware's obstacle course. If we want her to reach the\r\nend faster, we can't just sit and ruminate on canine physiology until\r\nenlightenment strikes. Instead, we need to ",(0,r.jsx)(n.em,{children:"observe"})," her performance, see where\r\nshe stumbles, and then find faster paths for her to take."]}),"\n",(0,r.jsxs)(n.p,{children:["Much like agility training is particular to one dog and one obstacle course, we\r\ncan't assume that our virtual machine optimizations will make ",(0,r.jsx)(n.em,{children:"all"})," Lox programs\r\nrun faster on ",(0,r.jsx)(n.em,{children:"all"})," hardware. Different Lox programs stress different areas of\r\nthe VM, and different architectures have their own strengths and weaknesses."]}),"\n",(0,r.jsx)(n.h3,{id:"benchmarks",children:"Benchmarks"}),"\n",(0,r.jsx)(n.p,{children:"When we add new functionality, we validate correctness by writing tests -- Lox\r\nprograms that use a feature and validate the VM's behavior. Tests pin down\r\nsemantics and ensure we don't break existing features when we add new ones. We\r\nhave similar needs when it comes to performance:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["How do we validate that an optimization ",(0,r.jsx)(n.em,{children:"does"})," improve performance, and by\r\nhow much?"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["How do we ensure that other unrelated changes don't ",(0,r.jsx)(n.em,{children:"regress"})," performance?"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The Lox programs we write to accomplish those goals are ",(0,r.jsx)(n.strong,{children:"benchmarks"}),". These\r\nare carefully crafted programs that stress some part of the language\r\nimplementation. They measure not ",(0,r.jsx)(n.em,{children:"what"})," the program does, but how ",(0,r.jsx)(n.span,{name:"much",children:(0,r.jsx)(n.em,{children:"long"})})," it takes to do it."]}),"\n",(0,r.jsxs)(n.aside,{name:"much",children:["\n",(0,r.jsx)(n.p,{children:"Most benchmarks measure running time. But, of course, you'll eventually find\r\nyourself needing to write benchmarks that measure memory allocation, how much\r\ntime is spent in the garbage collector, startup time, etc."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"By measuring the performance of a benchmark before and after a change, you can\r\nsee what your change does. When you land an optimization, all of the tests\r\nshould behave exactly the same as they did before, but hopefully the benchmarks\r\nrun faster."}),"\n",(0,r.jsxs)(n.p,{children:["Once you have an entire ",(0,r.jsx)(n.span,{name:"js",children:(0,r.jsx)(n.em,{children:"suite"})})," of benchmarks, you can\r\nmeasure not just ",(0,r.jsx)(n.em,{children:"that"})," an optimization changes performance, but on which\r\n",(0,r.jsx)(n.em,{children:"kinds"})," of code. Often you'll find that some benchmarks get faster while others\r\nget slower. Then you have to make hard decisions about what kinds of code your\r\nlanguage implementation optimizes for."]}),"\n",(0,r.jsx)(n.p,{children:"The suite of benchmarks you choose to write is a key part of that decision. In\r\nthe same way that your tests encode your choices around what correct behavior\r\nlooks like, your benchmarks are the embodiment of your priorities when it comes\r\nto performance. They will guide which optimizations you implement, so choose\r\nyour benchmarks carefully, and don't forget to periodically reflect on whether\r\nthey are helping you reach your larger goals."}),"\n",(0,r.jsxs)(n.aside,{name:"js",children:["\n",(0,r.jsx)(n.p,{children:"In the early proliferation of JavaScript VMs, the first widely used benchmark\r\nsuite was SunSpider from WebKit. During the browser wars, marketing folks used\r\nSunSpider results to claim their browser was fastest. That highly incentivized\r\nVM hackers to optimize to those benchmarks."}),"\n",(0,r.jsxs)(n.p,{children:["Unfortunately, SunSpider programs often didn't match real-world JavaScript. They\r\nwere mostly microbenchmarks -- tiny toy programs that completed quickly. Those\r\nbenchmarks penalize complex just-in-time compilers that start off slower but get\r\n",(0,r.jsx)(n.em,{children:"much"})," faster once the JIT has had enough time to optimize and re-compile hot\r\ncode paths. This put VM hackers in the unfortunate position of having to choose\r\nbetween making the SunSpider numbers get better, or actually optimizing the\r\nkinds of programs real users ran."]}),"\n",(0,r.jsx)(n.p,{children:"Google's V8 team responded by sharing their Octane benchmark suite, which was\r\ncloser to real-world code at the time. Years later, as JavaScript use patterns\r\ncontinued to evolve, even Octane outlived its usefulness. Expect that your\r\nbenchmarks will evolve as your language's ecosystem does."}),"\n",(0,r.jsxs)(n.p,{children:["Remember, the ultimate goal is to make ",(0,r.jsx)(n.em,{children:"user programs"})," faster, and benchmarks\r\nare only a proxy for that."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Benchmarking is a subtle art. Like tests, you need to balance not overfitting to\r\nyour implementation while ensuring that the benchmark does actually tickle the\r\ncode paths that you care about. When you measure performance, you need to\r\ncompensate for variance caused by CPU throttling, caching, and other weird\r\nhardware and operating system quirks. I won't give you a whole sermon here,\r\nbut treat benchmarking as its own skill that improves with practice."}),"\n",(0,r.jsx)(n.h3,{id:"profiling",children:"Profiling"}),"\n",(0,r.jsx)(n.p,{children:"OK, so you've got a few benchmarks now. You want to make them go faster. Now\r\nwhat? First of all, let's assume you've done all the obvious, easy work. You are\r\nusing the right algorithms and data structures -- or, at least, you aren't using\r\nones that are aggressively wrong. I don't consider using a hash table instead of\r\na linear search through a huge unsorted array \"optimization\" so much as \"good\r\nsoftware engineering\"."}),"\n",(0,r.jsxs)(n.p,{children:["Since the hardware is too complex to reason about our program's performance from\r\nfirst principles, we have to go out into the field. That means ",(0,r.jsx)(n.em,{children:"profiling"}),". A\r\n",(0,r.jsx)(n.strong,{children:"profiler"}),", if you've never used one, is a tool that runs your ",(0,r.jsx)(n.span,{name:"program",children:"program"})," and tracks hardware resource use as the code\r\nexecutes. Simple ones show you how much time was spent in each function in your\r\nprogram. Sophisticated ones log data cache misses, instruction cache misses,\r\nbranch mispredictions, memory allocations, and all sorts of other metrics."]}),"\n",(0,r.jsxs)(n.aside,{name:"program",children:["\n",(0,r.jsxs)(n.p,{children:['"Your program" here means the Lox VM itself running some ',(0,r.jsx)(n.em,{children:"other"})," Lox program. We\r\nare trying to optimize clox, not the user's Lox script. Of course, the choice of\r\nwhich Lox program to load into our VM will highly affect which parts of clox get\r\nstressed, which is why benchmarks are so important."]}),"\n",(0,r.jsxs)(n.p,{children:["A profiler ",(0,r.jsx)(n.em,{children:"won't"})," show us how much time is spent in each ",(0,r.jsx)(n.em,{children:"Lox"}),' function in the\r\nscript being run. We\'d have to write our own "Lox profiler" to do that, which is\r\nslightly out of scope for this book.']}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["There are many profilers out there for various operating systems and languages.\r\nOn whatever platform you program, it's worth getting familiar with a decent\r\nprofiler. You don't need to be a master. I have learned things within minutes of\r\nthrowing a program at a profiler that would have taken me ",(0,r.jsx)(n.em,{children:"days"})," to discover on\r\nmy own through trial and error. Profilers are wonderful, magical tools."]}),"\n",(0,r.jsx)(n.h2,{id:"faster-hash-table-probing",children:"Faster Hash Table Probing"}),"\n",(0,r.jsxs)(n.p,{children:["Enough pontificating, let's get some performance charts going up and to the\r\nright. The first optimization we'll do, it turns out, is about the ",(0,r.jsx)(n.em,{children:"tiniest"}),"\r\npossible change we could make to our VM."]}),"\n",(0,r.jsx)(n.p,{children:"When I first got the bytecode virtual machine that clox is descended from\r\nworking, I did what any self-respecting VM hacker would do. I cobbled together a\r\ncouple of benchmarks, fired up a profiler, and ran those scripts through my\r\ninterpreter. In a dynamically typed language like Lox, a large fraction of user\r\ncode is field accesses and method calls, so one of my benchmarks looked\r\nsomething like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-lox",children:"class Zoo {\r\n  init() {\r\n    this.aardvark = 1;\r\n    this.baboon   = 1;\r\n    this.cat      = 1;\r\n    this.donkey   = 1;\r\n    this.elephant = 1;\r\n    this.fox      = 1;\r\n  }\r\n  ant()    { return this.aardvark; }\r\n  banana() { return this.baboon; }\r\n  tuna()   { return this.cat; }\r\n  hay()    { return this.donkey; }\r\n  grass()  { return this.elephant; }\r\n  mouse()  { return this.fox; }\r\n}\r\n\r\nvar zoo = Zoo();\r\nvar sum = 0;\r\nvar start = clock();\r\nwhile (sum < 100000000) {\r\n  sum = sum + zoo.ant()\r\n            + zoo.banana()\r\n            + zoo.tuna()\r\n            + zoo.hay()\r\n            + zoo.grass()\r\n            + zoo.mouse();\r\n}\r\n\r\nprint clock() - start;\r\nprint sum;\n"})}),"\n",(0,r.jsxs)(n.aside,{name:"sum",className:"bottom",children:["\n",(0,r.jsxs)(n.p,{children:["Another thing this benchmark is careful to do is ",(0,r.jsx)(n.em,{children:"use"})," the result of the code it\r\nexecutes. By calculating a rolling sum and printing the result, we ensure the VM\r\n",(0,r.jsx)(n.em,{children:"must"})," execute all that Lox code. This is an important habit. Unlike our simple\r\nLox VM, many compilers do aggressive dead code elimination and are smart enough\r\nto discard a computation whose result is never used."]}),"\n",(0,r.jsx)(n.p,{children:"Many a programming language hacker has been impressed by the blazing performance\r\nof a VM on some benchmark, only to realize that it's because the compiler\r\noptimized the entire benchmark program away to nothing."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["If you've never seen a benchmark before, this might seem ludicrous. ",(0,r.jsx)(n.em,{children:"What"})," is\r\ngoing on here? The program itself doesn't intend to ",(0,r.jsx)(n.span,{name:"sum",children:"do"}),"\r\nanything useful. What it does do is call a bunch of methods and access a bunch\r\nof fields since those are the parts of the language we're interested in. Fields\r\nand methods live in hash tables, so it takes care to populate at least a ",(0,r.jsx)(n.span,{name:"more",children:(0,r.jsx)(n.em,{children:"few"})})," interesting keys in those tables. That is all wrapped\r\nin a big loop to ensure our profiler has enough execution time to dig in and see\r\nwhere the cycles are going."]}),"\n",(0,r.jsxs)(n.aside,{name:"more",children:["\n",(0,r.jsx)(n.p,{children:"If you really want to benchmark hash table performance, you should use many\r\ntables of different sizes. The six keys we add to each table here aren't even\r\nenough to get over our hash table's eight-element minimum threshold. But I\r\ndidn't want to throw an enormous benchmark script at you. Feel free to add more\r\ncritters and treats if you like."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Before I tell you what my profiler showed me, spend a minute taking a few\r\nguesses. Where in clox's codebase do you think the VM spent most of its time? Is\r\nthere any code we've written in previous chapters that you suspect is\r\nparticularly slow?"}),"\n",(0,r.jsxs)(n.p,{children:["Here's what I found: Naturally, the function with the greatest inclusive time is\r\n",(0,r.jsx)(n.code,{children:"run()"}),". (",(0,r.jsx)(n.strong,{children:"Inclusive time"})," means the total time spent in some function and all\r\nother functions it calls -- the total time between when you enter the function\r\nand when it returns.) Since ",(0,r.jsx)(n.code,{children:"run()"})," is the main bytecode execution loop, it\r\ndrives everything."]}),"\n",(0,r.jsxs)(n.p,{children:["Inside ",(0,r.jsx)(n.code,{children:"run()"}),", there are small chunks of time sprinkled in various cases in the\r\nbytecode switch for common instructions like ",(0,r.jsx)(n.code,{children:"OP_POP"}),", ",(0,r.jsx)(n.code,{children:"OP_RETURN"}),", and\r\n",(0,r.jsx)(n.code,{children:"OP_ADD"}),". The big heavy instructions are ",(0,r.jsx)(n.code,{children:"OP_GET_GLOBAL"})," with 17% of the\r\nexecution time, ",(0,r.jsx)(n.code,{children:"OP_GET_PROPERTY"})," at 12%, and ",(0,r.jsx)(n.code,{children:"OP_INVOKE"})," which takes a whopping\r\n42% of the total running time."]}),"\n",(0,r.jsxs)(n.p,{children:["So we've got three hotspots to optimize? Actually, no. Because it turns out\r\nthose three instructions spend almost all of their time inside calls to the same\r\nfunction: ",(0,r.jsx)(n.code,{children:"tableGet()"}),". That function claims a whole 72% of the execution time\r\n(again, inclusive). Now, in a dynamically typed language, we expect to spend a\r\nfair bit of time looking stuff up in hash tables -- it's sort of the price of\r\ndynamism. But, still, ",(0,r.jsx)(n.em,{children:"wow."})]}),"\n",(0,r.jsx)(n.h3,{id:"slow-key-wrapping",children:"Slow key wrapping"}),"\n",(0,r.jsxs)(n.p,{children:["If you take a look at ",(0,r.jsx)(n.code,{children:"tableGet()"}),", you'll see it's mostly a wrapper around a\r\ncall to ",(0,r.jsx)(n.code,{children:"findEntry()"})," where the actual hash table lookup happens. To refresh\r\nyour memory, here it is in full:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c",children:"static Entry* findEntry(Entry* entries, int capacity,\r\n                        ObjString* key) {\r\n  uint32_t index = key->hash % capacity;\r\n  Entry* tombstone = NULL;\r\n\r\n  for (;;) {\r\n    Entry* entry = &entries[index];\r\n    if (entry->key == NULL) {\r\n      if (IS_NIL(entry->value)) {\r\n        // Empty entry.\r\n        return tombstone != NULL ? tombstone : entry;\r\n      } else {\r\n        // We found a tombstone.\r\n        if (tombstone == NULL) tombstone = entry;\r\n      }\r\n    } else if (entry->key == key) {\r\n      // We found the key.\r\n      return entry;\r\n    }\r\n\r\n    index = (index + 1) % capacity;\r\n  }\r\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:["When running that previous benchmark -- on my machine, at least -- the VM spends\r\n70% of the total execution time on ",(0,r.jsx)(n.em,{children:"one line"})," in this function. Any guesses as\r\nto which one? No? It's this:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c",children:"  uint32_t index = key->hash % capacity;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["That pointer dereference isn't the problem. It's the little ",(0,r.jsx)(n.code,{children:"%"}),". It turns out\r\nthe modulo operator is ",(0,r.jsx)(n.em,{children:"really"})," slow. Much slower than other ",(0,r.jsx)(n.span,{name:"division",children:"arithmetic"})," operators. Can we do something better?"]}),"\n",(0,r.jsxs)(n.aside,{name:"division",children:["\n",(0,r.jsxs)(n.p,{children:["Pipelining makes it hard to talk about the performance of an individual CPU\r\ninstruction, but to give you a feel for things, division and modulo are about\r\n30-50 ",(0,r.jsx)(n.em,{children:"times"})," slower than addition and subtraction on x86."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"In the general case, it's really hard to re-implement a fundamental arithmetic\r\noperator in user code in a way that's faster than what the CPU itself can do.\r\nAfter all, our C code ultimately compiles down to the CPU's own arithmetic\r\noperations. If there were tricks we could use to go faster, the chip would\r\nalready be using them."}),"\n",(0,r.jsx)(n.p,{children:"However, we can take advantage of the fact that we know more about our problem\r\nthan the CPU does. We use modulo here to take a key string's hash code and\r\nwrap it to fit within the bounds of the table's entry array. That array starts\r\nout at eight elements and grows by a factor of two each time. We know -- and the\r\nCPU and C compiler do not -- that our table's size is always a power of two."}),"\n",(0,r.jsxs)(n.p,{children:["Because we're clever bit twiddlers, we know a faster way to calculate the\r\nremainder of a number modulo a power of two: ",(0,r.jsx)(n.strong,{children:"bit masking"}),". Let's say we want\r\nto calculate 229 modulo 64. The answer is 37, which is not particularly apparent\r\nin decimal, but is clearer when you view those numbers in binary:"]}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/mask.png",alt:"The bit patterns resulting from 229 % 64 = 37 and 229 & 63 = 37."}),"\n",(0,r.jsx)(n.p,{children:"On the left side of the illustration, notice how the result (37) is simply the\r\ndividend (229) with the highest two bits shaved off? Those two highest bits are\r\nthe bits at or to the left of the divisor's single 1 bit."}),"\n",(0,r.jsxs)(n.p,{children:["On the right side, we get the same result by taking 229 and bitwise ",(0,r.jsx)(n.span,{className:"small-caps",children:"AND"}),"-ing it with 63, which is one less than our\r\noriginal power of two divisor. Subtracting one from a power of two gives you a\r\nseries of 1 bits. That is exactly the mask we need in order to strip out those\r\ntwo leftmost bits."]}),"\n",(0,r.jsxs)(n.p,{children:["In other words, you can calculate a number modulo any power of two by simply\r\n",(0,r.jsx)(n.span,{className:"small-caps",children:"AND"}),"-ing it with that power of two minus one. I'm\r\nnot enough of a mathematician to ",(0,r.jsx)(n.em,{children:"prove"})," to you that this works, but if you\r\nthink it through, it should make sense. We can replace that slow modulo operator\r\nwith a very fast decrement and bitwise ",(0,r.jsx)(n.span,{className:"small-caps",children:"AND"}),". We\r\nsimply change the offending line of code to this:"]}),"\n",(0,r.jsx)(n.p,{children:"^code initial-index (2 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["CPUs love bitwise operators, so it's hard to ",(0,r.jsx)(n.span,{name:"sub",children:"improve"})," on that."]}),"\n",(0,r.jsxs)(n.aside,{name:"sub",children:["\n",(0,r.jsx)(n.p,{children:"Another potential improvement is to eliminate the decrement by storing the bit\r\nmask directly instead of the capacity. In my tests, that didn't make a\r\ndifference. Instruction pipelining makes some operations essentially free if the\r\nCPU is bottlenecked elsewhere."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Our linear probing search may need to wrap around the end of the array, so there\r\nis another modulo in ",(0,r.jsx)(n.code,{children:"findEntry()"})," to update."]}),"\n",(0,r.jsx)(n.p,{children:"^code next-index (4 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"This line didn't show up in the profiler since most searches don't wrap."}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"findEntry()"})," function has a sister function, ",(0,r.jsx)(n.code,{children:"tableFindString()"})," that does\r\na hash table lookup for interning strings. We may as well apply the same\r\noptimizations there too. This function is called only when interning strings,\r\nwhich wasn't heavily stressed by our benchmark. But a Lox program that created\r\nlots of strings might noticeably benefit from this change."]}),"\n",(0,r.jsx)(n.p,{children:"^code find-string-index (2 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"And also when the linear probing wraps around."}),"\n",(0,r.jsx)(n.p,{children:"^code find-string-next (3 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Let's see if our fixes were worth it. I tweaked that zoological benchmark to\r\ncount how many ",(0,r.jsx)(n.span,{name:"batch",children:"batches"})," of 10,000 calls it can run in\r\nten seconds. More batches equals faster performance. On my machine using the\r\nunoptimized code, the benchmark gets through 3,192 batches. After this\r\noptimization, that jumps to 6,249."]}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/hash-chart.png",alt:"Bar chart comparing the performance before and after the optimization."}),"\n",(0,r.jsx)(n.p,{children:"That's almost exactly twice as much work in the same amount of time. We made the\r\nVM twice as fast (usual caveat: on this benchmark). That is a massive win when\r\nit comes to optimization. Usually you feel good if you can claw a few percentage\r\npoints here or there. Since methods, fields, and global variables are so\r\nprevalent in Lox programs, this tiny optimization improves performance across\r\nthe board. Almost every Lox program benefits."}),"\n",(0,r.jsxs)(n.aside,{name:"batch",children:["\n",(0,r.jsxs)(n.p,{children:["Our original benchmark fixed the amount of ",(0,r.jsx)(n.em,{children:"work"})," and then measured the ",(0,r.jsx)(n.em,{children:"time"}),".\r\nChanging the script to count how many batches of calls it can do in ten seconds\r\nfixes the time and measures the work. For performance comparisons, I like the\r\nlatter measure because the reported number represents ",(0,r.jsx)(n.em,{children:"speed"}),". You can directly\r\ncompare the numbers before and after an optimization. When measuring execution\r\ntime, you have to do a little arithmetic to get to a good relative measure of\r\nperformance."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Now, the point of this section is ",(0,r.jsx)(n.em,{children:"not"})," that the modulo operator is profoundly\r\nevil and you should stamp it out of every program you ever write. Nor is it that\r\nmicro-optimization is a vital engineering skill. It's rare that a performance\r\nproblem has such a narrow, effective solution. We got lucky."]}),"\n",(0,r.jsxs)(n.p,{children:["The point is that we didn't ",(0,r.jsx)(n.em,{children:"know"})," that the modulo operator was a performance\r\ndrain until our profiler told us so. If we had wandered around our VM's codebase\r\nblindly guessing at hotspots, we likely wouldn't have noticed it. What I want\r\nyou to take away from this is how important it is to have a profiler in your\r\ntoolbox."]}),"\n",(0,r.jsxs)(n.p,{children:["To reinforce that point, let's go ahead and run the original benchmark in our\r\nnow-optimized VM and see what the profiler shows us. On my machine, ",(0,r.jsx)(n.code,{children:"tableGet()"}),"\r\nis still a fairly large chunk of execution time. That's to be expected for a\r\ndynamically typed language. But it has dropped from 72% of the total execution\r\ntime down to 35%. That's much more in line with what we'd like to see and shows\r\nthat our optimization didn't just make the program faster, but made it faster\r\n",(0,r.jsx)(n.em,{children:"in the way we expected"}),". Profilers are as useful for verifying solutions as\r\nthey are for discovering problems."]}),"\n",(0,r.jsx)(n.h2,{id:"nan-boxing",children:"NaN Boxing"}),"\n",(0,r.jsxs)(n.p,{children:["This next optimization has a very different feel. Thankfully, despite the odd\r\nname, it does not involve punching your grandmother. It's different, but not,\r\nlike, ",(0,r.jsx)(n.em,{children:"that"})," different. With our previous optimization, the profiler told us\r\nwhere the problem was, and we merely had to use some ingenuity to come up with a\r\nsolution."]}),"\n",(0,r.jsxs)(n.p,{children:["This optimization is more subtle, and its performance effects more scattered\r\nacross the virtual machine. The profiler won't help us come up with this.\r\nInstead, it was invented by ",(0,r.jsx)(n.span,{name:"someone",children:"someone"})," thinking deeply\r\nabout the lowest levels of machine architecture."]}),"\n",(0,r.jsxs)(n.aside,{name:"someone",children:["\n",(0,r.jsx)(n.p,{children:'I\'m not sure who first came up with this trick. The earliest source I can find\r\nis David Gudeman\'s 1993 paper "Representing Type Information in Dynamically\r\nTyped Languages". Everyone else cites that. But Gudeman himself says the paper\r\nisn\'t novel work, but instead "gathers together a body of folklore".'}),"\n",(0,r.jsx)(n.p,{children:"Maybe the inventor has been lost to the mists of time, or maybe it's been\r\nreinvented a number of times. Anyone who ruminates on IEEE 754 long enough\r\nprobably starts thinking about trying to stuff something useful into all those\r\nunused NaN bits."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Like the heading says, this optimization is called ",(0,r.jsx)(n.strong,{children:"NaN boxing"})," or sometimes\r\n",(0,r.jsx)(n.strong,{children:"NaN tagging"}),'. Personally I like the latter name because "boxing" tends to imply\r\nsome kind of heap-allocated representation, but the former seems to be the more\r\nwidely used term. This technique changes how we represent values in the VM.']}),"\n",(0,r.jsx)(n.p,{children:"On a 64-bit machine, our Value type takes up 16 bytes. The struct has two\r\nfields, a type tag and a union for the payload. The largest fields in the union\r\nare an Obj pointer and a double, which are both 8 bytes. To keep the union field\r\naligned to an 8-byte boundary, the compiler adds padding after the tag too:"}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/union.png",alt:"Byte layout of the 16-byte tagged union Value."}),"\n",(0,r.jsxs)(n.p,{children:["That's pretty big. If we could cut that down, then the VM could pack more values\r\ninto the same amount of memory. Most computers have plenty of RAM these days, so\r\nthe direct memory savings aren't a huge deal. But a smaller representation means\r\nmore Values fit in a cache line. That means fewer cache misses, which affects\r\n",(0,r.jsx)(n.em,{children:"speed"}),"."]}),"\n",(0,r.jsx)(n.p,{children:'If Values need to be aligned to their largest payload size, and a Lox number or\r\nObj pointer needs a full 8 bytes, how can we get any smaller? In a dynamically\r\ntyped language like Lox, each value needs to carry not just its payload, but\r\nenough additional information to determine the value\'s type at runtime. If a Lox\r\nnumber is already using the full 8 bytes, where could we squirrel away a couple\r\nof extra bits to tell the runtime "this is a number"?'}),"\n",(0,r.jsxs)(n.p,{children:["This is one of the perennial problems for dynamic language hackers. It\r\nparticularly bugs them because statically typed languages don't generally have\r\nthis problem. The type of each value is known at compile time, so no extra\r\nmemory is needed at runtime to track it. When your C compiler compiles a 32-bit\r\nint, the resulting variable gets ",(0,r.jsx)(n.em,{children:"exactly"})," 32 bits of storage."]}),"\n",(0,r.jsx)(n.p,{children:"Dynamic language folks hate losing ground to the static camp, so they've come up\r\nwith a number of very clever ways to pack type information and a payload into a\r\nsmall number of bits. NaN boxing is one of those. It's a particularly good fit\r\nfor languages like JavaScript and Lua, where all numbers are double-precision\r\nfloating point. Lox is in that same boat."}),"\n",(0,r.jsx)(n.h3,{id:"what-is-and-is-not-a-number",children:"What is (and is not) a number?"}),"\n",(0,r.jsxs)(n.p,{children:["Before we start optimizing, we need to really understand how our friend the CPU\r\nrepresents floating-point numbers. Almost all machines today use the same\r\nscheme, encoded in the venerable scroll ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/IEEE_754",children:"IEEE 754"}),', known to mortals as the\r\n"IEEE Standard for Floating-Point Arithmetic".']}),"\n",(0,r.jsxs)(n.p,{children:["In the eyes of your computer, a ",(0,r.jsx)(n.span,{name:"hyphen",children:"64-bit"}),",\r\ndouble-precision, IEEE floating-point number looks like this:"]}),"\n",(0,r.jsxs)(n.aside,{name:"hyphen",children:["\n",(0,r.jsx)(n.p,{children:"That's a lot of hyphens for one sentence."}),"\n"]}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/double.png",alt:"Bit representation of an IEEE 754 double."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Starting from the right, the first 52 bits are the ",(0,r.jsx)(n.strong,{children:"fraction"}),",\r\n",(0,r.jsx)(n.strong,{children:"mantissa"}),", or ",(0,r.jsx)(n.strong,{children:"significand"})," bits. They represent the significant digits\r\nof the number, as a binary integer."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Next to that are 11 ",(0,r.jsx)(n.strong,{children:"exponent"})," bits. These tell you how far the mantissa\r\nis shifted away from the decimal (well, binary) point."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The highest bit is the ",(0,r.jsx)(n.span,{name:"sign",children:(0,r.jsx)(n.strong,{children:"sign bit"})}),", which\r\nindicates whether the number is positive or negative."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"I know that's a little vague, but this chapter isn't a deep dive on\r\nfloating point representation. If you want to know how the exponent and mantissa\r\nplay together, there are already better explanations out there than I could\r\nwrite."}),"\n",(0,r.jsxs)(n.aside,{name:"sign",children:["\n",(0,r.jsx)(n.p,{children:'Since the sign bit is always present, even if the number is zero, that implies\r\nthat "positive zero" and "negative zero" have different bit representations, and\r\nindeed, IEEE 754 does distinguish those.'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:['The important part for our purposes is that the spec carves out a special case\r\nexponent. When all of the exponent bits are set, then instead of just\r\nrepresenting a really big number, the value has a different meaning. These\r\nvalues are "Not a Number" (hence, ',(0,r.jsx)(n.strong,{children:"NaN"}),") values. They represent concepts like\r\ninfinity or the result of division by zero."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Any"})," double whose exponent bits are all set is a NaN, regardless of the\r\nmantissa bits. That means there's lots and lots of ",(0,r.jsx)(n.em,{children:"different"})," NaN bit patterns.\r\nIEEE 754 divides those into two categories. Values where the highest mantissa\r\nbit is 0 are called ",(0,r.jsx)(n.strong,{children:"signalling NaNs"}),", and the others are ",(0,r.jsx)(n.strong,{children:"quiet NaNs"}),".\r\nSignalling NaNs are intended to be the result of erroneous computations, like\r\ndivision by zero. A chip ",(0,r.jsx)(n.span,{name:"abort",children:"may"})," detect when one of these\r\nvalues is produced and abort a program completely. They may self-destruct if you\r\ntry to read one."]}),"\n",(0,r.jsxs)(n.aside,{name:"abort",children:["\n",(0,r.jsxs)(n.p,{children:["I don't know if any CPUs actually ",(0,r.jsx)(n.em,{children:"do"})," trap signalling NaNs and abort. The spec\r\njust says they ",(0,r.jsx)(n.em,{children:"could"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Quiet NaNs are supposed to be safer to use. They don't represent useful numeric\r\nvalues, but they should at least not set your hand on fire if you touch them."}),"\n",(0,r.jsx)(n.p,{children:"Every double with all of its exponent bits set and its highest mantissa bit set\r\nis a quiet NaN. That leaves 52 bits unaccounted for. We'll avoid one of those so\r\nthat we don't step on Intel's \"QNaN Floating-Point Indefinite\" value, leaving us\r\n51 bits. Those remaining bits can be anything. We're talking\r\n2,251,799,813,685,248 unique quiet NaN bit patterns."}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/nan.png",alt:"The bits in a double that make it a quiet NaN."}),"\n",(0,r.jsxs)(n.p,{children:["This means a 64-bit double has enough room to store all of the various different\r\nnumeric floating-point values and ",(0,r.jsx)(n.em,{children:"also"})," has room for another 51 bits of data\r\nthat we can use however we want. That's plenty of room to set aside a couple of\r\nbit patterns to represent Lox's ",(0,r.jsx)(n.code,{children:"nil"}),", ",(0,r.jsx)(n.code,{children:"true"}),", and ",(0,r.jsx)(n.code,{children:"false"})," values. But what\r\nabout Obj pointers? Don't pointers need a full 64 bits too?"]}),"\n",(0,r.jsxs)(n.p,{children:["Fortunately, we have another trick up our other sleeve. Yes, technically\r\npointers on a 64-bit architecture are 64 bits. But, no architecture I know of\r\nactually uses that entire address space. Instead, most widely used chips today\r\nonly ever use the low ",(0,r.jsx)(n.span,{name:"48",children:"48"})," bits. The remaining 16 bits are\r\neither unspecified or always zero."]}),"\n",(0,r.jsxs)(n.aside,{name:"48",children:["\n",(0,r.jsx)(n.p,{children:"48 bits is enough to address 262,144 gigabytes of memory. Modern operating\r\nsystems also give each process its own address space, so that should be plenty."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["If we've got 51 bits, we can stuff a 48-bit pointer in there with three bits to\r\nspare. Those three bits are just enough to store tiny type tags to distinguish\r\nbetween ",(0,r.jsx)(n.code,{children:"nil"}),", Booleans, and Obj pointers."]}),"\n",(0,r.jsx)(n.p,{children:"That's NaN boxing. Within a single 64-bit double, you can store all of the\r\ndifferent floating-point numeric values, a pointer, or any of a couple of other\r\nspecial sentinel values. Half the memory usage of our current Value struct,\r\nwhile retaining all of the fidelity."}),"\n",(0,r.jsxs)(n.p,{children:["What's particularly nice about this representation is that there is no need to\r\n",(0,r.jsx)(n.em,{children:"convert"}),' a numeric double value into a "boxed" form. Lox numbers ',(0,r.jsx)(n.em,{children:"are"})," just\r\nnormal, 64-bit doubles. We still need to ",(0,r.jsx)(n.em,{children:"check"}),' their type before we use them,\r\nsince Lox is dynamically typed, but we don\'t need to do any bit shifting or\r\npointer indirection to go from "value" to "number".']}),"\n",(0,r.jsx)(n.p,{children:"For the other value types, there is a conversion step, of course. But,\r\nfortunately, our VM hides all of the mechanism to go from values to raw types\r\nbehind a handful of macros. Rewrite those to implement NaN boxing, and the rest\r\nof the VM should just work."}),"\n",(0,r.jsx)(n.h3,{id:"conditional-support",children:"Conditional support"}),"\n",(0,r.jsx)(n.p,{children:"I know the details of this new representation aren't clear in your head yet.\r\nDon't worry, they will crystallize as we work through the implementation. Before\r\nwe get to that, we're going to put some compile-time scaffolding in place."}),"\n",(0,r.jsxs)(n.p,{children:["For our previous optimization, we rewrote the previous slow code and called it\r\ndone. This one is a little different. NaN boxing relies on some very low-level\r\ndetails of how a chip represents floating-point numbers and pointers. It\r\n",(0,r.jsx)(n.em,{children:"probably"})," works on most CPUs you're likely to encounter, but you can never be\r\ntotally sure."]}),"\n",(0,r.jsxs)(n.p,{children:["It would suck if our VM completely lost support for an architecture just because\r\nof its value representation. To avoid that, we'll maintain support for ",(0,r.jsx)(n.em,{children:"both"}),"\r\nthe old tagged union implementation of Value and the new NaN-boxed form. We\r\nselect which representation we want at compile time using this flag:"]}),"\n",(0,r.jsx)(n.p,{children:"^code define-nan-boxing (2 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"If that's defined, the VM uses the new form. Otherwise, it reverts to the old\r\nstyle. The few pieces of code that care about the details of the value\r\nrepresentation -- mainly the handful of macros for wrapping and unwrapping\r\nValues -- vary based on whether this flag is set. The rest of the VM can\r\ncontinue along its merry way."}),"\n",(0,r.jsx)(n.p,{children:'Most of the work happens in the "value" module where we add a section for the\r\nnew type.'}),"\n",(0,r.jsx)(n.p,{children:"^code nan-boxing (2 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"When NaN boxing is enabled, the actual type of a Value is a flat, unsigned\r\n64-bit integer. We could use double instead, which would make the macros for\r\ndealing with Lox numbers a little simpler. But all of the other macros need to\r\ndo bitwise operations and uint64_t is a much friendlier type for that. Outside\r\nof this module, the rest of the VM doesn't really care one way or the other."}),"\n",(0,r.jsxs)(n.p,{children:["Before we start re-implementing those macros, we close the ",(0,r.jsx)(n.code,{children:"#else"})," branch of the\r\n",(0,r.jsx)(n.code,{children:"#ifdef"})," at the end of the definitions for the old representation."]}),"\n",(0,r.jsx)(n.p,{children:"^code end-if-nan-boxing (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Our remaining task is simply to fill in that first ",(0,r.jsx)(n.code,{children:"#ifdef"})," section with new\r\nimplementations of all the stuff already in the ",(0,r.jsx)(n.code,{children:"#else"})," side. We'll work through\r\nit one value type at a time, from easiest to hardest."]}),"\n",(0,r.jsx)(n.h3,{id:"numbers",children:"Numbers"}),"\n",(0,r.jsx)(n.p,{children:"We'll start with numbers since they have the most direct representation under\r\nNaN boxing. To \"convert\" a C double to a NaN-boxed clox Value, we don't need to\r\ntouch a single bit -- the representation is exactly the same. But we do need to\r\nconvince our C compiler of that fact, which we made harder by defining Value to\r\nbe uint64_t."}),"\n",(0,r.jsxs)(n.p,{children:["We need to get the compiler to take a set of bits that it thinks are a double\r\nand use those same bits as a uint64_t, or vice versa. This is called ",(0,r.jsx)(n.strong,{children:"type\r\npunning"}),". C and C++ programmers have been doing this since the days of bell\r\nbottoms and 8-tracks, but the language specifications have ",(0,r.jsx)(n.span,{name:"hesitate",children:"hesitated"})," to say which of the many ways to do this is\r\nofficially sanctioned."]}),"\n",(0,r.jsxs)(n.aside,{name:"hesitate",className:"bottom",children:["\n",(0,r.jsx)(n.p,{children:"Spec authors don't like type punning because it makes optimization harder. A key\r\noptimization technique is reordering instructions to fill the CPU's execution\r\npipelines. A compiler can reorder code only when doing so doesn't have a\r\nuser-visible effect, obviously."}),"\n",(0,r.jsxs)(n.p,{children:["Pointers make that harder. If two pointers point to the same value, then a write\r\nthrough one and a read through the other cannot be reordered. But what about two\r\npointers of ",(0,r.jsx)(n.em,{children:"different"})," types? If those could point to the same object, then\r\nbasically ",(0,r.jsx)(n.em,{children:"any"})," two pointers could be aliases to the same value. That\r\ndrastically limits the amount of code the compiler is free to rearrange."]}),"\n",(0,r.jsxs)(n.p,{children:["To avoid that, compilers want to assume ",(0,r.jsx)(n.strong,{children:"strict aliasing"})," -- pointers of\r\nincompatible types cannot point to the same value. Type punning, by nature,\r\nbreaks that assumption."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["I know one way to convert a ",(0,r.jsx)(n.code,{children:"double"})," to ",(0,r.jsx)(n.code,{children:"Value"})," and back that I believe is\r\nsupported by both the C and C++ specs. Unfortunately, it doesn't fit in a single\r\nexpression, so the conversion macros have to call out to helper functions.\r\nHere's the first macro:"]}),"\n",(0,r.jsx)(n.p,{children:"^code number-val (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"That macro passes the double here:"}),"\n",(0,r.jsx)(n.p,{children:"^code num-to-value (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["I know, weird, right? The way to treat a series of bytes as having a different\r\ntype without changing their value at all is ",(0,r.jsx)(n.code,{children:"memcpy()"}),"? This looks horrendously\r\nslow: Create a local variable. Pass its address to the operating system through\r\na syscall to copy a few bytes. Then return the result, which is the exact same\r\nbytes as the input. Thankfully, because this ",(0,r.jsx)(n.em,{children:"is"})," the supported idiom for type\r\npunning, most compilers recognize the pattern and optimize away the ",(0,r.jsx)(n.code,{children:"memcpy()"}),"\r\nentirely."]}),"\n",(0,r.jsx)(n.p,{children:'"Unwrapping" a Lox number is the mirror image.'}),"\n",(0,r.jsx)(n.p,{children:"^code as-number (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"That macro calls this function:"}),"\n",(0,r.jsx)(n.p,{children:"^code value-to-num (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["It works exactly the same except we swap the types. Again, the compiler will\r\neliminate all of it. Even though those calls to\r\n",(0,r.jsx)(n.code,{children:"memcpy()"})," will disappear, we still need to show the compiler ",(0,r.jsx)(n.em,{children:"which"})," ",(0,r.jsx)(n.code,{children:"memcpy()"}),"\r\nwe're calling so we also need an ",(0,r.jsx)(n.span,{name:"union",children:"include"}),"."]}),"\n",(0,r.jsxs)(n.aside,{name:"union",className:"bottom",children:["\n",(0,r.jsxs)(n.p,{children:["If you find yourself with a compiler that does not optimize the ",(0,r.jsx)(n.code,{children:"memcpy()"})," away,\r\ntry this instead:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c",children:"double valueToNum(Value value) {\r\n  union {\r\n    uint64_t bits;\r\n    double num;\r\n  } data;\r\n  data.bits = value;\r\n  return data.num;\r\n}\n"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"^code include-string (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["That was a lot of code to ultimately do nothing but silence the C type checker.\r\nDoing a runtime type ",(0,r.jsx)(n.em,{children:"test"})," on a Lox number is a little more interesting. If all\r\nwe have are exactly the bits for a double, how do we tell that it ",(0,r.jsx)(n.em,{children:"is"})," a double?\r\nIt's time to get bit twiddling."]}),"\n",(0,r.jsx)(n.p,{children:"^code is-number (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["We know that every Value that is ",(0,r.jsx)(n.em,{children:"not"})," a number will use a special quiet NaN\r\nrepresentation. And we presume we have correctly avoided any of the meaningful\r\nNaN representations that may actually be produced by doing arithmetic on\r\nnumbers."]}),"\n",(0,r.jsxs)(n.p,{children:["If the double has all of its NaN bits set, and the quiet NaN bit set, and one\r\nmore for good measure, we can be ",(0,r.jsx)(n.span,{name:"certain",children:"pretty certain"})," it\r\nis one of the bit patterns we ourselves have set aside for other types. To check\r\nthat, we mask out all of the bits except for our set of quiet NaN bits. If ",(0,r.jsx)(n.em,{children:"all"}),"\r\nof those bits are set, it must be a NaN-boxed value of some other Lox type.\r\nOtherwise, it is actually a number."]}),"\n",(0,r.jsxs)(n.aside,{name:"certain",children:["\n",(0,r.jsx)(n.p,{children:"Pretty certain, but not strictly guaranteed. As far as I know, there is nothing\r\npreventing a CPU from producing a NaN value as the result of some operation\r\nwhose bit representation collides with ones we have claimed. But in my tests\r\nacross a number of architectures, I haven't seen it happen."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"The set of quiet NaN bits are declared like this:"}),"\n",(0,r.jsx)(n.p,{children:"^code qnan (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"It would be nice if C supported binary literals. But if you do the conversion,\r\nyou'll see that value is the same as this:"}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/qnan.png",alt:"The quiet NaN bits."}),"\n",(0,r.jsx)(n.p,{children:"This is exactly all of the exponent bits, plus the quiet NaN bit, plus one extra\r\nto dodge that Intel value."}),"\n",(0,r.jsx)(n.h3,{id:"nil-true-and-false",children:"Nil, true, and false"}),"\n",(0,r.jsxs)(n.p,{children:["The next type to handle is ",(0,r.jsx)(n.code,{children:"nil"}),". That's pretty simple since there's only one\r\n",(0,r.jsx)(n.code,{children:"nil"})," value and thus we need only a single bit pattern to represent it. There\r\nare two other singleton values, the two Booleans, ",(0,r.jsx)(n.code,{children:"true"})," and ",(0,r.jsx)(n.code,{children:"false"}),". This calls\r\nfor three total unique bit patterns."]}),"\n",(0,r.jsx)(n.p,{children:'Two bits give us four different combinations, which is plenty. We claim the two\r\nlowest bits of our unused mantissa space as a "type tag" to determine which of\r\nthese three singleton values we\'re looking at. The three type tags are defined\r\nlike so:'}),"\n",(0,r.jsx)(n.p,{children:"^code tags (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Our representation of ",(0,r.jsx)(n.code,{children:"nil"})," is thus all of the bits required to define our\r\nquiet NaN representation along with the ",(0,r.jsx)(n.code,{children:"nil"})," type tag bits:"]}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/nil.png",alt:"The bit representation of the nil value."}),"\n",(0,r.jsx)(n.p,{children:"In code, we check the bits like so:"}),"\n",(0,r.jsx)(n.p,{children:"^code nil-val (2 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["We simply bitwise ",(0,r.jsx)(n.span,{className:"small-caps",children:"OR"})," the quiet NaN bits and the\r\ntype tag, and then do a little cast dance to teach the C compiler what we want\r\nthose bits to mean."]}),"\n",(0,r.jsxs)(n.p,{children:["Since ",(0,r.jsx)(n.code,{children:"nil"})," has only a single bit representation, we can use equality on\r\nuint64_t to see if a Value is ",(0,r.jsx)(n.code,{children:"nil"}),"."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.span,{name:"equal"})}),"\n",(0,r.jsx)(n.p,{children:"^code is-nil (2 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["You can guess how we define the ",(0,r.jsx)(n.code,{children:"true"})," and ",(0,r.jsx)(n.code,{children:"false"})," values."]}),"\n",(0,r.jsx)(n.p,{children:"^code false-true-vals (2 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"The bits look like this:"}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/bools.png",alt:"The bit representation of the true and false values."}),"\n",(0,r.jsx)(n.p,{children:"To convert a C bool into a Lox Boolean, we rely on these two singleton values\r\nand the good old conditional operator."}),"\n",(0,r.jsx)(n.p,{children:"^code bool-val (2 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"There's probably a cleverer bitwise way to do this, but my hunch is that the\r\ncompiler can figure one out faster than I can. Going the other direction is\r\nsimpler."}),"\n",(0,r.jsx)(n.p,{children:"^code as-bool (2 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:['Since we know there are exactly two Boolean bit representations in Lox -- unlike\r\nin C where any non-zero value can be considered "true" -- if it ain\'t ',(0,r.jsx)(n.code,{children:"true"}),", it\r\nmust be ",(0,r.jsx)(n.code,{children:"false"}),". This macro does assume you call it only on a Value that you\r\nknow ",(0,r.jsx)(n.em,{children:"is"})," a Lox Boolean. To check that, there's one more macro."]}),"\n",(0,r.jsx)(n.p,{children:"^code is-bool (2 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"That looks a little strange. A more obvious macro would look like this:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c",children:"#define IS_BOOL(v) ((v) == TRUE_VAL || (v) == FALSE_VAL)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Unfortunately, that's not safe. The expansion mentions ",(0,r.jsx)(n.code,{children:"v"})," twice, which means if\r\nthat expression has any side effects, they will be executed twice. We could have\r\nthe macro call out to a separate function, but, ugh, what a chore."]}),"\n",(0,r.jsxs)(n.p,{children:["Instead, we bitwise ",(0,r.jsx)(n.span,{className:"small-caps",children:"OR"})," a 1 onto the value to\r\nmerge the only two valid Boolean bit patterns. That leaves three potential\r\nstates the value can be in:"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["It was ",(0,r.jsx)(n.code,{children:"FALSE_VAL"})," and has now been converted to ",(0,r.jsx)(n.code,{children:"TRUE_VAL"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["It was ",(0,r.jsx)(n.code,{children:"TRUE_VAL"})," and the ",(0,r.jsx)(n.code,{children:"| 1"})," did nothing and it's still ",(0,r.jsx)(n.code,{children:"TRUE_VAL"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"It's some other, non-Boolean value."}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["At that point, we can simply compare the result to ",(0,r.jsx)(n.code,{children:"TRUE_VAL"})," to see if we're\r\nin the first two states or the third."]}),"\n",(0,r.jsx)(n.h3,{id:"objects",children:"Objects"}),"\n",(0,r.jsxs)(n.p,{children:["The last value type is the hardest. Unlike the singleton values, there are\r\nbillions of different pointer values we need to box inside a NaN. This means we\r\nneed both some kind of tag to indicate that these particular NaNs ",(0,r.jsx)(n.em,{children:"are"})," Obj\r\npointers, and room for the addresses themselves."]}),"\n",(0,r.jsxs)(n.p,{children:["The tag bits we used for the singleton values are in the region where I decided\r\nto store the pointer itself, so we can't easily use a different ",(0,r.jsx)(n.span,{name:"ptr",children:"bit"})," there to indicate that the value is an object reference.\r\nHowever, there is another bit we aren't using. Since all our NaN values are not\r\nnumbers -- it's right there in the name -- the sign bit isn't used for anything.\r\nWe'll go ahead and use that as the type tag for objects. If one of our quiet\r\nNaNs has its sign bit set, then it's an Obj pointer. Otherwise, it must be one\r\nof the previous singleton values."]}),"\n",(0,r.jsxs)(n.aside,{name:"ptr",children:["\n",(0,r.jsxs)(n.p,{children:["We actually ",(0,r.jsx)(n.em,{children:"could"})," use the lowest bits to store the type tag even when the\r\nvalue is an Obj pointer. That's because Obj pointers are always aligned to an\r\n8-byte boundary since Obj contains a 64-bit field. That, in turn, implies that\r\nthe three lowest bits of an Obj pointer will always be zero. We could store\r\nwhatever we wanted in there and just mask it off before dereferencing the\r\npointer."]}),"\n",(0,r.jsxs)(n.p,{children:["This is another value representation optimization called ",(0,r.jsx)(n.strong,{children:"pointer tagging"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"If the sign bit is set, then the remaining low bits store the pointer to the\r\nObj:"}),"\n",(0,r.jsx)(n.img,{src:"image/optimization/obj.png",alt:"Bit representation of an Obj* stored in a Value."}),"\n",(0,r.jsx)(n.p,{children:"To convert a raw Obj pointer to a Value, we take the pointer and set all of the\r\nquiet NaN bits and the sign bit."}),"\n",(0,r.jsx)(n.p,{children:"^code obj-val (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["The pointer itself is a full 64 bits, and in ",(0,r.jsx)(n.span,{name:"safe",children:"principle"}),",\r\nit could thus overlap with some of those quiet NaN and sign bits. But in\r\npractice, at least on the architectures I've tested, everything above the 48th\r\nbit in a pointer is always zero. There's a lot of casting going on here, which\r\nI've found is necessary to satisfy some of the pickiest C compilers, but the\r\nend result is just jamming some bits together."]}),"\n",(0,r.jsxs)(n.aside,{name:"safe",children:["\n",(0,r.jsxs)(n.p,{children:["I try to follow the letter of the law when it comes to the code in this book, so\r\nthis paragraph is dubious. There comes a point when optimizing where you push\r\nthe boundary of not just what the ",(0,r.jsx)(n.em,{children:"spec says"})," you can do, but what a real\r\ncompiler and chip let you get away with."]}),"\n",(0,r.jsx)(n.p,{children:"There are risks when stepping outside of the spec, but there are rewards in that\r\nlawless territory too. It's up to you to decide if the gains are worth it."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We define the sign bit like so:"}),"\n",(0,r.jsx)(n.p,{children:"^code sign-bit (2 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"To get the Obj pointer back out, we simply mask off all of those extra bits."}),"\n",(0,r.jsx)(n.p,{children:"^code as-obj (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["The tilde (",(0,r.jsx)(n.code,{children:"~"}),"), if you haven't done enough bit manipulation to encounter it\r\nbefore, is bitwise ",(0,r.jsx)(n.span,{className:"small-caps",children:"NOT"}),". It toggles all ones and\r\nzeroes in its operand. By masking the value with the bitwise negation of the\r\nquiet NaN and sign bits, we ",(0,r.jsx)(n.em,{children:"clear"})," those bits and let the pointer bits remain."]}),"\n",(0,r.jsx)(n.p,{children:"One last macro:"}),"\n",(0,r.jsx)(n.p,{children:"^code is-obj (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"A Value storing an Obj pointer has its sign bit set, but so does any negative\r\nnumber. To tell if a Value is an Obj pointer, we need to check that both the\r\nsign bit and all of the quiet NaN bits are set. This is similar to how we detect\r\nthe type of the singleton values, except this time we use the sign bit as the\r\ntag."}),"\n",(0,r.jsx)(n.h3,{id:"value-functions",children:"Value functions"}),"\n",(0,r.jsx)(n.p,{children:'The rest of the VM usually goes through the macros when working with Values, so\r\nwe are almost done. However, there are a couple of functions in the "value"\r\nmodule that peek inside the otherwise black box of Value and work with its\r\nencoding directly. We need to fix those too.'}),"\n",(0,r.jsxs)(n.p,{children:["The first is ",(0,r.jsx)(n.code,{children:"printValue()"}),". It has separate code for each value type. We no\r\nlonger have an explicit type enum we can switch on, so instead we use a series\r\nof type tests to handle each kind of value."]}),"\n",(0,r.jsx)(n.p,{children:"^code print-value (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"This is technically a tiny bit slower than a switch, but compared to the\r\noverhead of actually writing to a stream, it's negligible."}),"\n",(0,r.jsxs)(n.p,{children:["We still support the original tagged union representation, so we keep the old\r\ncode and enclose it in the ",(0,r.jsx)(n.code,{children:"#else"})," conditional section."]}),"\n",(0,r.jsx)(n.p,{children:"^code end-print-value (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"The other operation is testing two values for equality."}),"\n",(0,r.jsx)(n.p,{children:"^code values-equal (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"It doesn't get much simpler than that! If the two bit representations are\r\nidentical, the values are equal. That does the right thing for the singleton\r\nvalues since each has a unique bit representation and they are only equal to\r\nthemselves. It also does the right thing for Obj pointers, since objects use\r\nidentity for equality -- two Obj references are equal only if they point to the\r\nexact same object."}),"\n",(0,r.jsxs)(n.p,{children:["It's ",(0,r.jsx)(n.em,{children:"mostly"})," correct for numbers too. Most floating-point numbers with\r\ndifferent bit representations are distinct numeric values. Alas, IEEE 754\r\ncontains a pothole to trip us up. For reasons that aren't entirely clear to me,\r\nthe spec mandates that NaN values are ",(0,r.jsx)(n.em,{children:"not"})," equal to ",(0,r.jsx)(n.em,{children:"themselves"}),". This isn't a\r\nproblem for the special quiet NaNs that we are using for our own purposes. But\r\nit's possible to produce a \"real\" arithmetic NaN in Lox, and if we want to\r\ncorrectly implement IEEE 754 numbers, then the resulting value is not supposed\r\nto be equal to itself. More concretely:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-lox",children:"var nan = 0/0;\r\nprint nan == nan;\n"})}),"\n",(0,r.jsxs)(n.p,{children:['IEEE 754 says this program is supposed to print "false". It does the right thing\r\nwith our old tagged union representation because the ',(0,r.jsx)(n.code,{children:"VAL_NUMBER"})," case applies\r\n",(0,r.jsx)(n.code,{children:"=="})," to two values that the C compiler knows are doubles. Thus the compiler\r\ngenerates the right CPU instruction to perform an IEEE floating-point equality."]}),"\n",(0,r.jsxs)(n.p,{children:["Our new representation breaks that by defining Value to be a uint64_t. If we\r\nwant to be ",(0,r.jsx)(n.em,{children:"fully"})," compliant with IEEE 754, we need to handle this case."]}),"\n",(0,r.jsx)(n.p,{children:"^code nan-equality (1 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["I know, it's weird. And there is a performance cost to doing this type test\r\nevery time we check two Lox values for equality. If we are willing to sacrifice\r\na little ",(0,r.jsx)(n.span,{name:"java",children:"compatibility"})," -- who ",(0,r.jsx)(n.em,{children:"really"})," cares if NaN is\r\nnot equal to itself? -- we could leave this off. I'll leave it up to you to\r\ndecide how pedantic you want to be."]}),"\n",(0,r.jsxs)(n.aside,{name:"java",children:["\n",(0,r.jsxs)(n.p,{children:["In fact, jlox gets NaN equality wrong. Java does the right thing when you\r\ncompare primitive doubles using ",(0,r.jsx)(n.code,{children:"=="}),", but not if you box those to Double or\r\nObject and compare them using ",(0,r.jsx)(n.code,{children:"equals()"}),", which is how jlox implements equality."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Finally, we close the conditional compilation section around the old\r\nimplementation."}),"\n",(0,r.jsx)(n.p,{children:"^code end-values-equal (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"And that's it. This optimization is complete, as is our clox virtual machine.\r\nThat was the last line of new code in the book."}),"\n",(0,r.jsx)(n.h3,{id:"evaluating-performance",children:"Evaluating performance"}),"\n",(0,r.jsx)(n.p,{children:"The code is done, but we still need to figure out if we actually made anything\r\nbetter with these changes. Evaluating an optimization like this is very\r\ndifferent from the previous one. There, we had a clear hotspot visible in the\r\nprofiler. We fixed that part of the code and could instantly see the hotspot\r\nget faster."}),"\n",(0,r.jsxs)(n.p,{children:["The effects of changing the value representation are more diffused. The macros\r\nare expanded in place wherever they are used, so the performance changes are\r\nspread across the codebase in a way that's hard for many profilers to track\r\nwell, especially in an ",(0,r.jsx)(n.span,{name:"opt",children:"optimized"})," build."]}),"\n",(0,r.jsxs)(n.aside,{name:"opt",children:["\n",(0,r.jsx)(n.p,{children:'When doing profiling work, you almost always want to profile an optimized\r\n"release" build of your program since that reflects the performance story your\r\nend users experience. Compiler optimizations, like inlining, can dramatically\r\naffect which parts of the code are performance hotspots. Hand-optimizing a debug\r\nbuild risks sending you off "fixing" problems that the optimizing compiler will\r\nalready solve for you.'}),"\n",(0,r.jsx)(n.p,{children:"Make sure you don't accidentally benchmark and optimize your debug build. I seem\r\nto make that mistake at least once a year."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["We also can't easily ",(0,r.jsx)(n.em,{children:"reason"})," about the effects of our change. We've made values\r\nsmaller, which reduces cache misses all across the VM. But the actual real-world\r\nperformance effect of that change is highly dependent on the memory use of the\r\nLox program being run. A tiny Lox microbenchmark may not have enough values\r\nscattered around in memory for the effect to be noticeable, and even things like\r\nthe addresses handed out to us by the C memory allocator can impact the results."]}),"\n",(0,r.jsxs)(n.p,{children:["If we did our job right, basically everything gets a little faster, especially\r\non larger, more complex Lox programs. But it is possible that the extra bitwise\r\noperations we do when NaN-boxing values nullify the gains from the better\r\nmemory use. Doing performance work like this is unnerving because you can't\r\neasily ",(0,r.jsx)(n.em,{children:"prove"})," that you've made the VM better. You can't point to a single\r\nsurgically targeted microbenchmark and say, \"There, see?\""]}),"\n",(0,r.jsxs)(n.p,{children:["Instead, what we really need is a ",(0,r.jsx)(n.em,{children:"suite"})," of larger benchmarks. Ideally, they\r\nwould be distilled from real-world applications -- not that such a thing exists\r\nfor a toy language like Lox. Then we can measure the aggregate performance\r\nchanges across all of those. I did my best to cobble together a handful of\r\nlarger Lox programs. On my machine, the new value representation seems to make\r\neverything roughly 10% faster across the board."]}),"\n",(0,r.jsxs)(n.p,{children:["That's not a huge improvement, especially compared to the profound effect of\r\nmaking hash table lookups faster. I added this optimization in large part\r\nbecause it's a good example of a certain ",(0,r.jsx)(n.em,{children:"kind"})," of performance work you may\r\nexperience, and honestly, because I think it's technically really cool. It might\r\nnot be the first thing I would reach for if I were seriously trying to make clox\r\nfaster. There is probably other, lower-hanging fruit."]}),"\n",(0,r.jsx)(n.p,{children:"But, if you find yourself working on a program where all of the easy wins have\r\nbeen taken, then at some point you may want to think about tuning your value\r\nrepresentation. I hope this chapter has shined a light on some of the options\r\nyou have in that area."}),"\n",(0,r.jsx)(n.h2,{id:"where-to-next",children:"Where to Next"}),"\n",(0,r.jsx)(n.p,{children:"We'll stop here with the Lox language and our two interpreters. We could tinker\r\non it forever, adding new language features and clever speed improvements. But,\r\nfor this book, I think we've reached a natural place to call our work complete.\r\nI won't rehash everything we've learned in the past many pages. You were there\r\nwith me and you remember. Instead, I'd like to take a minute to talk about where\r\nyou might go from here. What is the next step in your programming language\r\njourney?"}),"\n",(0,r.jsxs)(n.p,{children:["Most of you probably won't spend a significant part of your career working in\r\ncompilers or interpreters. It's a pretty small slice of the computer science\r\nacademia pie, and an even smaller segment of software engineering in industry.\r\nThat's OK. Even if you never work on a compiler again in your life, you will\r\ncertainly ",(0,r.jsx)(n.em,{children:"use"})," one, and I hope this book has equipped you with a better\r\nunderstanding of how the programming languages you use are designed and\r\nimplemented."]}),"\n",(0,r.jsx)(n.p,{children:"You have also learned a handful of important, fundamental data structures and\r\ngotten some practice doing low-level profiling and optimization work. That kind\r\nof expertise is helpful no matter what domain you program in."}),"\n",(0,r.jsxs)(n.p,{children:["I also hope I gave you a new way of ",(0,r.jsx)(n.span,{name:"domain",children:"looking"})," at and\r\nsolving problems. Even if you never work on a language again, you may be\r\nsurprised to discover how many programming problems can be seen as\r\nlanguage-",(0,r.jsx)(n.em,{children:"like"}),'. Maybe that report generator you need to write can be modeled as\r\na series of stack-based "instructions" that the generator "executes". That user\r\ninterface you need to render looks an awful lot like traversing an AST.']}),"\n",(0,r.jsxs)(n.aside,{name:"domain",children:["\n",(0,r.jsx)(n.p,{children:"This goes for other domains too. I don't think there's a single topic I've\r\nlearned in programming -- or even outside of programming -- that I haven't ended\r\nup finding useful in other areas. One of my favorite aspects of software\r\nengineering is how much it rewards those with eclectic interests."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"If you do want to go further down the programming language rabbit hole, here\r\nare some suggestions for which branches in the tunnel to explore:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Our simple, single-pass bytecode compiler pushed us towards mostly runtime\r\noptimization. In a mature language implementation, compile-time optimization\r\nis generally more important, and the field of compiler optimizations is\r\nincredibly rich. Grab a classic ",(0,r.jsx)(n.span,{name:"cooper",children:"compilers"})," book,\r\nand rebuild the front end of clox or jlox to be a sophisticated compilation\r\npipeline with some interesting intermediate representations and optimization\r\npasses."]}),"\n",(0,r.jsx)(n.p,{children:"Dynamic typing will place some restrictions on how far you can go, but there\r\nis still a lot you can do. Or maybe you want to take a big leap and add\r\nstatic types and a type checker to Lox. That will certainly give your front\r\nend a lot more to chew on."}),"\n",(0,r.jsxs)(n.aside,{name:"cooper",children:["\n",(0,r.jsxs)(n.p,{children:["I like Cooper and Torczon's ",(0,r.jsx)(n.em,{children:"Engineering a Compiler"})," for this. Appel's\r\n",(0,r.jsx)(n.em,{children:"Modern Compiler Implementation"})," books are also well regarded."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["In this book, I aim to be correct, but not particularly rigorous. My goal is\r\nmostly to give you an ",(0,r.jsx)(n.em,{children:"intuition"})," and a feel for doing language work. If you\r\nlike more precision, then the whole world of programming language academia\r\nis waiting for you. Languages and compilers have been studied formally since\r\nbefore we even had computers, so there is no shortage of books and papers on\r\nparser theory, type systems, semantics, and formal logic. Going down this\r\npath will also teach you how to read CS papers, which is a valuable skill in\r\nits own right."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Or, if you just really enjoy hacking on and making languages, you can take\r\nLox and turn it into your own ",(0,r.jsx)(n.span,{name:"license",children:"plaything"}),". Change\r\nthe syntax to something that delights your eye. Add missing features or\r\nremove ones you don't like. Jam new optimizations in there."]}),"\n",(0,r.jsxs)(n.aside,{name:"license",children:["\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.em,{children:"text"})," of this book is copyrighted to me, but the ",(0,r.jsx)(n.em,{children:"code"})," and the\r\nimplementations of jlox and clox use the very permissive ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/MIT_License",children:"MIT license"}),".\r\nYou are more than welcome to ",(0,r.jsx)(n.a,{href:"https://github.com/munificent/craftinginterpreters",children:"take either of those interpreters"})," and\r\ndo whatever you want with them. Go to town."]}),"\n",(0,r.jsx)(n.p,{children:'If you make significant changes to the language, it would be good to also\r\nchange the name, mostly to avoid confusing people about what the name "Lox"\r\nrepresents.'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Eventually you may get to a point where you have something you think others\r\ncould use as well. That gets you into the very distinct world of programming\r\nlanguage ",(0,r.jsx)(n.em,{children:"popularity"}),". Expect to spend a ton of time writing documentation,\r\nexample programs, tools, and useful libraries. The field is crowded with\r\nlanguages vying for users. To thrive in that space you'll have to put on\r\nyour marketing hat and ",(0,r.jsx)(n.em,{children:"sell"}),". Not everyone enjoys that kind of\r\npublic-facing work, but if you do, it can be incredibly gratifying to see\r\npeople use your language to express themselves."]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Or maybe this book has satisfied your craving and you'll stop here. Whichever\r\nway you go, or don't go, there is one lesson I hope to lodge in your heart. Like\r\nI was, you may have initially been intimidated by programming languages. But in\r\nthese chapters, you've seen that even really challenging material can be tackled\r\nby us mortals if we get our hands dirty and take it a step at a time. If you can\r\nhandle compilers and interpreters, you can do anything you put your mind to."}),"\n",(0,r.jsxs)(n.div,{className:"challenges",children:["\n",(0,r.jsx)(n.h2,{id:"challenges",children:"Challenges"}),"\n",(0,r.jsx)(n.p,{children:"Assigning homework on the last day of school seems cruel but if you really want\r\nsomething to do during your summer vacation:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Fire up your profiler, run a couple of benchmarks, and look for other\r\nhotspots in the VM. Do you see anything in the runtime that you can improve?"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Many strings in real-world user programs are small, often only a character\r\nor two. This is less of a concern in clox because we intern strings, but\r\nmost VMs don't. For those that don't, heap allocating a tiny character array\r\nfor each of those little strings and then representing the value as a\r\npointer to that array is wasteful. Often, the pointer is larger than the\r\nstring's characters. A classic trick is to have a separate value\r\nrepresentation for small strings that stores the characters inline in the\r\nvalue."}),"\n",(0,r.jsx)(n.p,{children:"Starting from clox's original tagged union representation, implement that\r\noptimization. Write a couple of relevant benchmarks and see if it helps."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Reflect back on your experience with this book. What parts of it worked well\r\nfor you? What didn't? Was it easier for you to learn bottom-up or top-down?\r\nDid the illustrations help or distract? Did the analogies clarify or\r\nconfuse?"}),"\n",(0,r.jsx)(n.p,{children:"The more you understand your personal learning style, the more effectively\r\nyou can upload knowledge into your head. You can specifically target\r\nmaterial that teaches you the way you learn best."}),"\n"]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>s});var r=t(6540);const i={},o=r.createContext(i);function a(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);