"use strict";(self.webpackChunkmybooks=self.webpackChunkmybooks||[]).push([[4354],{6517:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>a,toc:()=>h});var r=t(4848),i=t(8453);const o={},s=void 0,a={id:"Craftinginterpreters/not-translated-yet/chunks-of-bytecode",title:"chunks-of-bytecode",description:"If you find that you're spending almost all your time on theory, start turning",source:"@site/docs/Craftinginterpreters/not-translated-yet/chunks-of-bytecode.md",sourceDirName:"Craftinginterpreters/not-translated-yet",slug:"/Craftinginterpreters/not-translated-yet/chunks-of-bytecode",permalink:"/docs/Craftinginterpreters/not-translated-yet/chunks-of-bytecode",draft:!1,unlisted:!1,editUrl:"https://github.com/jabberwocky238/jabberwocky238.github.io/docs/Craftinginterpreters/not-translated-yet/chunks-of-bytecode.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"calls-and-functions",permalink:"/docs/Craftinginterpreters/not-translated-yet/calls-and-functions"},next:{title:"classes-and-instances",permalink:"/docs/Craftinginterpreters/not-translated-yet/classes-and-instances"}},l={},h=[{value:"Bytecode?",id:"bytecode",level:2},{value:"Why not walk the AST?",id:"why-not-walk-the-ast",level:3},{value:"Why not compile to native code?",id:"why-not-compile-to-native-code",level:3},{value:"What is bytecode?",id:"what-is-bytecode",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Chunks of Instructions",id:"chunks-of-instructions",level:2},{value:"A dynamic array of instructions",id:"a-dynamic-array-of-instructions",level:3},{value:"Disassembling Chunks",id:"disassembling-chunks",level:2},{value:"Constants",id:"constants",level:2},{value:"Representing values",id:"representing-values",level:3},{value:"Value arrays",id:"value-arrays",level:3},{value:"Constant instructions",id:"constant-instructions",level:3},{value:"Line Information",id:"line-information",level:2},{value:"Disassembling line information",id:"disassembling-line-information",level:3},{value:"Challenges",id:"challenges",level:2},{value:"Design Note: Test Your Language",id:"design-note-test-your-language",level:2}];function c(e){const n={a:"a",aside:"aside",blockquote:"blockquote",cite:"cite",code:"code",div:"div",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"If you find that you're spending almost all your time on theory, start turning\r\nsome attention to practical things; it will improve your theories. If you find\r\nthat you're spending almost all your time on practice, start turning some\r\nattention to theoretical things; it will improve your practice."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.cite,{children:"Donald Knuth"})}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["We already have ourselves a complete implementation of Lox with jlox, so why\r\nisn't the book over yet? Part of this is because jlox relies on the ",(0,r.jsx)(n.span,{name:"metal",children:"JVM"})," to do lots of things for us. If we want to understand\r\nhow an interpreter works all the way down to the metal, we need to build those\r\nbits and pieces ourselves."]}),"\n",(0,r.jsxs)(n.aside,{name:"metal",children:["\n",(0,r.jsxs)(n.p,{children:["Of course, our second interpreter relies on the C standard library for basics\r\nlike memory allocation, and the C compiler frees us from details of the\r\nunderlying machine code we're running it on. Heck, that machine code is probably\r\nimplemented in terms of microcode on the chip. And the C runtime relies on the\r\noperating system to hand out pages of memory. But we have to stop ",(0,r.jsx)(n.em,{children:"somewhere"})," if\r\nthis book is going to fit on your bookshelf."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"An even more fundamental reason that jlox isn't sufficient is that it's too damn\r\nslow. A tree-walk interpreter is fine for some kinds of high-level, declarative\r\nlanguages. But for a general-purpose, imperative language -- even a \"scripting\"\r\nlanguage like Lox -- it won't fly. Take this little script:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-lox",children:"fun fib(n) {\r\n  if (n < 2) return n;\r\n  return fib(n - 1) + fib(n - 2); // [fib]\r\n}\r\n\r\nvar before = clock();\r\nprint fib(40);\r\nvar after = clock();\r\nprint after - before;\n"})}),"\n",(0,r.jsxs)(n.aside,{name:"fib",children:["\n",(0,r.jsxs)(n.p,{children:["This is a comically inefficient way to actually calculate Fibonacci numbers.\r\nOur goal is to see how fast the ",(0,r.jsx)(n.em,{children:"interpreter"})," runs, not to see how fast of a\r\nprogram we can write. A slow program that does a lot of work -- pointless or not\r\n-- is a good test case for that."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["On my laptop, that takes jlox about 72 seconds to execute. An equivalent C\r\nprogram finishes in half a second. Our dynamically typed scripting language is\r\nnever going to be as fast as a statically typed language with manual memory\r\nmanagement, but we don't need to settle for more than ",(0,r.jsx)(n.em,{children:"two orders of magnitude"}),"\r\nslower."]}),"\n",(0,r.jsx)(n.p,{children:"We could take jlox and run it in a profiler and start tuning and tweaking\r\nhotspots, but that will only get us so far. The execution model -- walking the\r\nAST -- is fundamentally the wrong design. We can't micro-optimize that to the\r\nperformance we want any more than you can polish an AMC Gremlin into an SR-71\r\nBlackbird."}),"\n",(0,r.jsx)(n.p,{children:"We need to rethink the core model. This chapter introduces that model, bytecode,\r\nand begins our new interpreter, clox."}),"\n",(0,r.jsx)(n.h2,{id:"bytecode",children:"Bytecode?"}),"\n",(0,r.jsx)(n.p,{children:"In engineering, few choices are without trade-offs. To best understand why we're\r\ngoing with bytecode, let's stack it up against a couple of alternatives."}),"\n",(0,r.jsx)(n.h3,{id:"why-not-walk-the-ast",children:"Why not walk the AST?"}),"\n",(0,r.jsx)(n.p,{children:"Our existing interpreter has a couple of things going for it:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Well, first, we already wrote it. It's done. And the main reason it's done\r\nis because this style of interpreter is ",(0,r.jsx)(n.em,{children:"really simple to implement"}),". The\r\nruntime representation of the code directly maps to the syntax. It's\r\nvirtually effortless to get from the parser to the data structures we need\r\nat runtime."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["It's ",(0,r.jsx)(n.em,{children:"portable"}),". Our current interpreter is written in Java and runs on any\r\nplatform Java supports. We could write a new implementation in C using the\r\nsame approach and compile and run our language on basically every platform\r\nunder the sun."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Those are real advantages. But, on the other hand, it's ",(0,r.jsx)(n.em,{children:"not memory-efficient"}),".\r\nEach piece of syntax becomes an AST node. A tiny Lox expression like ",(0,r.jsx)(n.code,{children:"1 + 2"}),"\r\nturns into a slew of objects with lots of pointers between them, something like:"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.span,{name:"header"})}),"\n",(0,r.jsxs)(n.aside,{name:"header",children:["\n",(0,r.jsx)(n.p,{children:'The "(header)" parts are the bookkeeping information the Java virtual machine\r\nuses to support memory management and store the object\'s type. Those take up\r\nspace too!'}),"\n"]}),"\n",(0,r.jsx)(n.img,{src:"image/chunks-of-bytecode/ast.png",alt:"The tree of Java objects created to represent '1 + 2'."}),"\n",(0,r.jsxs)(n.p,{children:["Each of those pointers adds an extra 32 or 64 bits of overhead to the object.\r\nWorse, sprinkling our data across the heap in a loosely connected web of objects\r\ndoes bad things for ",(0,r.jsx)(n.span,{name:"locality",children:(0,r.jsx)(n.em,{children:"spatial locality"})}),"."]}),"\n",(0,r.jsxs)(n.aside,{name:"locality",children:["\n",(0,r.jsxs)(n.p,{children:["I wrote ",(0,r.jsx)(n.a,{href:"http://gameprogrammingpatterns.com/data-locality.html",children:"an entire chapter"})," about this exact problem in my first\r\nbook, ",(0,r.jsx)(n.em,{children:"Game Programming Patterns"}),", if you want to really dig in."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Modern CPUs process data way faster than they can pull it from RAM. To\r\ncompensate for that, chips have multiple layers of caching. If a piece of memory\r\nit needs is already in the cache, it can be loaded more quickly. We're talking\r\nupwards of 100 ",(0,r.jsx)(n.em,{children:"times"})," faster."]}),"\n",(0,r.jsx)(n.p,{children:"How does data get into that cache? The machine speculatively stuffs things in\r\nthere for you. Its heuristic is pretty simple. Whenever the CPU reads a bit of\r\ndata from RAM, it pulls in a whole little bundle of adjacent bytes and stuffs\r\nthem in the cache."}),"\n",(0,r.jsxs)(n.p,{children:["If our program next requests some data close enough to be inside that cache\r\nline, our CPU runs like a well-oiled conveyor belt in a factory. We ",(0,r.jsx)(n.em,{children:"really"}),"\r\nwant to take advantage of this. To use the cache effectively, the way we\r\nrepresent code in memory should be dense and ordered like it's read."]}),"\n",(0,r.jsxs)(n.p,{children:["Now look up at that tree. Those sub-objects could be ",(0,r.jsx)(n.span,{name:"anywhere",children:(0,r.jsx)(n.em,{children:"anywhere"})}),". Every step the tree-walker takes where it\r\nfollows a reference to a child node may step outside the bounds of the cache and\r\nforce the CPU to stall until a new lump of data can be slurped in from RAM. Just\r\nthe ",(0,r.jsx)(n.em,{children:"overhead"})," of those tree nodes with all of their pointer fields and object\r\nheaders tends to push objects away from each other and out of the cache."]}),"\n",(0,r.jsxs)(n.aside,{name:"anywhere",children:["\n",(0,r.jsx)(n.p,{children:"Even if the objects happened to be allocated in sequential memory when the\r\nparser first produced them, after a couple of rounds of garbage collection --\r\nwhich may move objects around in memory -- there's no telling where they'll be."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Our AST walker has other overhead too around interface dispatch and the Visitor\r\npattern, but the locality issues alone are enough to justify a better code\r\nrepresentation."}),"\n",(0,r.jsx)(n.h3,{id:"why-not-compile-to-native-code",children:"Why not compile to native code?"}),"\n",(0,r.jsxs)(n.p,{children:["If you want to go ",(0,r.jsx)(n.em,{children:"real"})," fast, you want to get all of those layers of\r\nindirection out of the way. Right down to the metal. Machine code. It even\r\n",(0,r.jsx)(n.em,{children:"sounds"})," fast. ",(0,r.jsx)(n.em,{children:"Machine code."})]}),"\n",(0,r.jsxs)(n.p,{children:["Compiling directly to the native instruction set the chip supports is what the\r\nfastest languages do. Targeting native code has been the most efficient option\r\nsince way back in the early days when engineers actually ",(0,r.jsx)(n.span,{name:"hand",children:"handwrote"})," programs in machine code."]}),"\n",(0,r.jsxs)(n.aside,{name:"hand",children:["\n",(0,r.jsxs)(n.p,{children:["Yes, they actually wrote machine code by hand. On punched cards. Which,\r\npresumably, they punched ",(0,r.jsx)(n.em,{children:"with their fists"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'If you\'ve never written any machine code, or its slightly more human-palatable\r\ncousin assembly code before, I\'ll give you the gentlest of introductions. Native\r\ncode is a dense series of operations, encoded directly in binary. Each\r\ninstruction is between one and a few bytes long, and is almost mind-numbingly\r\nlow level. "Move a value from this address to this register." "Add the integers\r\nin these two registers." Stuff like that.'}),"\n",(0,r.jsx)(n.p,{children:"The CPU cranks through the instructions, decoding and executing each one in\r\norder. There is no tree structure like our AST, and control flow is handled by\r\njumping from one point in the code directly to another. No indirection, no\r\noverhead, no unnecessary skipping around or chasing pointers."}),"\n",(0,r.jsx)(n.p,{children:"Lightning fast, but that performance comes at a cost. First of all, compiling to\r\nnative code ain't easy. Most chips in wide use today have sprawling Byzantine\r\narchitectures with heaps of instructions that accreted over decades. They\r\nrequire sophisticated register allocation, pipelining, and instruction\r\nscheduling."}),"\n",(0,r.jsxs)(n.p,{children:["And, of course, you've thrown ",(0,r.jsx)(n.span,{name:"back",children:"portability"})," out. Spend a\r\nfew years mastering some architecture and that still only gets you onto ",(0,r.jsx)(n.em,{children:"one"})," of\r\nthe several popular instruction sets out there. To get your language on all of\r\nthem, you need to learn all of their instruction sets and write a separate back\r\nend for each one."]}),"\n",(0,r.jsxs)(n.aside,{name:"back",children:["\n",(0,r.jsx)(n.p,{children:"The situation isn't entirely dire. A well-architected compiler lets you\r\nshare the front end and most of the middle layer optimization passes across the\r\ndifferent architectures you support. It's mainly the code generation and some of\r\nthe details around instruction selection that you'll need to write afresh each\r\ntime."}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.a,{href:"https://llvm.org/",children:"LLVM"})," project gives you some of this out of the box. If your compiler\r\noutputs LLVM's own special intermediate language, LLVM in turn compiles that to\r\nnative code for a plethora of architectures."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"what-is-bytecode",children:"What is bytecode?"}),"\n",(0,r.jsxs)(n.p,{children:["Fix those two points in your mind. On one end, a tree-walk interpreter is\r\nsimple, portable, and slow. On the other, native code is complex and\r\nplatform-specific but fast. Bytecode sits in the middle. It retains the\r\nportability of a tree-walker -- we won't be getting our hands dirty with\r\nassembly code in this book. It sacrifices ",(0,r.jsx)(n.em,{children:"some"})," simplicity to get a performance\r\nboost in return, though not as fast as going fully native."]}),"\n",(0,r.jsx)(n.p,{children:"Structurally, bytecode resembles machine code. It's a dense, linear sequence of\r\nbinary instructions. That keeps overhead low and plays nice with the cache.\r\nHowever, it's a much simpler, higher-level instruction set than any real chip\r\nout there. (In many bytecode formats, each instruction is only a single byte\r\nlong, hence \"bytecode\".)"}),"\n",(0,r.jsx)(n.p,{children:"Imagine you're writing a native compiler from some source language and you're\r\ngiven carte blanche to define the easiest possible architecture to target.\r\nBytecode is kind of like that. It's an idealized fantasy instruction set that\r\nmakes your life as the compiler writer easier."}),"\n",(0,r.jsxs)(n.p,{children:["The problem with a fantasy architecture, of course, is that it doesn't exist. We\r\nsolve that by writing an ",(0,r.jsx)(n.em,{children:"emulator"})," -- a simulated chip written in software that\r\ninterprets the bytecode one instruction at a time. A ",(0,r.jsx)(n.em,{children:"virtual machine (VM)"}),", if\r\nyou will."]}),"\n",(0,r.jsxs)(n.p,{children:["That emulation layer adds ",(0,r.jsx)(n.span,{name:"p-code",children:"overhead"}),", which is a key\r\nreason bytecode is slower than native code. But in return, it gives us\r\nportability. Write our VM in a language like C that is already supported on all\r\nthe machines we care about, and we can run our emulator on top of any hardware\r\nwe like."]}),"\n",(0,r.jsxs)(n.aside,{name:"p-code",children:["\n",(0,r.jsxs)(n.p,{children:["One of the first bytecode formats was ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/P-code_machine",children:"p-code"}),', developed for Niklaus Wirth\'s\r\nPascal language. You might think a PDP-11 running at 15MHz couldn\'t afford the\r\noverhead of emulating a virtual machine. But back then, computers were in their\r\nCambrian explosion and new architectures appeared every day. Keeping up with the\r\nlatest chips was worth more than squeezing the maximum performance from each\r\none. That\'s why the "p" in p-code doesn\'t stand for "Pascal", but "portable".']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This is the path we'll take with our new interpreter, clox. We'll follow in the\r\nfootsteps of the main implementations of Python, Ruby, Lua, OCaml, Erlang, and\r\nothers. In many ways, our VM's design will parallel the structure of our\r\nprevious interpreter:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"image/chunks-of-bytecode/phases.png",alt:"Phases of the two\nimplementations. jlox is Parser to Syntax Trees to Interpreter. clox is Compiler\nto Bytecode to Virtual Machine."})}),"\n",(0,r.jsx)(n.p,{children:"Of course, we won't implement the phases strictly in order. Like our previous\r\ninterpreter, we'll bounce around, building up the implementation one language\r\nfeature at a time. In this chapter, we'll get the skeleton of the application in\r\nplace and create the data structures needed to store and represent a chunk of\r\nbytecode."}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsxs)(n.p,{children:["Where else to begin, but at ",(0,r.jsx)(n.code,{children:"main()"}),"? ",(0,r.jsx)(n.span,{name:"ready",children:"Fire"})," up your\r\ntrusty text editor and start typing."]}),"\n",(0,r.jsxs)(n.aside,{name:"ready",children:["\n",(0,r.jsx)(n.p,{children:"Now is a good time to stretch, maybe crack your knuckles. A little montage music\r\nwouldn't hurt either."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"^code main-c"}),"\n",(0,r.jsx)(n.p,{children:"From this tiny seed, we will grow our entire VM. Since C provides us with so\r\nlittle, we first need to spend some time amending the soil. Some of that goes\r\ninto this header:"}),"\n",(0,r.jsx)(n.p,{children:"^code common-h"}),"\n",(0,r.jsxs)(n.p,{children:["There are a handful of types and constants we'll use throughout the interpreter,\r\nand this is a convenient place to put them. For now, it's the venerable ",(0,r.jsx)(n.code,{children:"NULL"}),",\r\n",(0,r.jsx)(n.code,{children:"size_t"}),", the nice C99 Boolean ",(0,r.jsx)(n.code,{children:"bool"}),", and explicit-sized integer types --\r\n",(0,r.jsx)(n.code,{children:"uint8_t"})," and friends."]}),"\n",(0,r.jsx)(n.h2,{id:"chunks-of-instructions",children:"Chunks of Instructions"}),"\n",(0,r.jsx)(n.p,{children:"Next, we need a module to define our code representation. I've been using\r\n\"chunk\" to refer to sequences of bytecode, so let's make that the official name\r\nfor that module."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-h"}),"\n",(0,r.jsxs)(n.p,{children:["In our bytecode format, each instruction has a one-byte ",(0,r.jsx)(n.strong,{children:"operation code"}),"\r\n(universally shortened to ",(0,r.jsx)(n.strong,{children:"opcode"}),"). That number controls what kind of\r\ninstruction we're dealing with -- add, subtract, look up variable, etc. We\r\ndefine those here:"]}),"\n",(0,r.jsx)(n.p,{children:"^code op-enum (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["For now, we start with a single instruction, ",(0,r.jsx)(n.code,{children:"OP_RETURN"}),". When we have a\r\nfull-featured VM, this instruction will mean \"return from the current function\".\r\nI admit this isn't exactly useful yet, but we have to start somewhere, and this\r\nis a particularly simple instruction, for reasons we'll get to later."]}),"\n",(0,r.jsx)(n.h3,{id:"a-dynamic-array-of-instructions",children:"A dynamic array of instructions"}),"\n",(0,r.jsx)(n.p,{children:"Bytecode is a series of instructions. Eventually, we'll store some other data\r\nalong with the instructions, so let's go ahead and create a struct to hold it\r\nall."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-struct (1 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["At the moment, this is simply a wrapper around an array of bytes. Since we don't\r\nknow how big the array needs to be before we start compiling a chunk, it must be\r\ndynamic. Dynamic arrays are one of my favorite data structures. That sounds like\r\nclaiming vanilla is my favorite ice cream ",(0,r.jsx)(n.span,{name:"flavor",children:"flavor"}),", but\r\nhear me out. Dynamic arrays provide:"]}),"\n",(0,r.jsxs)(n.aside,{name:"flavor",children:["\n",(0,r.jsx)(n.p,{children:"Butter pecan is actually my favorite."}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Cache-friendly, dense storage"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Constant-time indexed element lookup"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Constant-time appending to the end of the array"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:'Those features are exactly why we used dynamic arrays all the time in jlox under\r\nthe guise of Java\'s ArrayList class. Now that we\'re in C, we get to roll our\r\nown. If you\'re rusty on dynamic arrays, the idea is pretty simple. In addition\r\nto the array itself, we keep two numbers: the number of elements in the array we\r\nhave allocated ("capacity") and how many of those allocated entries are actually\r\nin use ("count").'}),"\n",(0,r.jsx)(n.p,{children:"^code count-and-capacity (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"When we add an element, if the count is less than the capacity, then there is\r\nalready available space in the array. We store the new element right in there\r\nand bump the count."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"image/chunks-of-bytecode/insert.png",alt:"Storing an element in an\narray that has enough capacity."})}),"\n",(0,r.jsx)(n.p,{children:"If we have no spare capacity, then the process is a little more involved."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"image/chunks-of-bytecode/grow.png",alt:"Growing the dynamic array\nbefore storing an element.",className:"wide"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.span,{name:"amortized",children:"Allocate"})," a new array with more capacity."]}),"\n",(0,r.jsx)(n.li,{children:"Copy the existing elements from the old array to the new one."}),"\n",(0,r.jsxs)(n.li,{children:["Store the new ",(0,r.jsx)(n.code,{children:"capacity"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Delete the old array."}),"\n",(0,r.jsxs)(n.li,{children:["Update ",(0,r.jsx)(n.code,{children:"code"})," to point to the new array."]}),"\n",(0,r.jsx)(n.li,{children:"Store the element in the new array now that there is room."}),"\n",(0,r.jsxs)(n.li,{children:["Update the ",(0,r.jsx)(n.code,{children:"count"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.aside,{name:"amortized",children:["\n",(0,r.jsxs)(n.p,{children:["Copying the existing elements when you grow the array makes it seem like\r\nappending an element is ",(0,r.jsx)(n.em,{children:"O(n)"}),", not ",(0,r.jsx)(n.em,{children:"O(1)"})," like I said above. However, you need\r\nto do this copy step only on ",(0,r.jsx)(n.em,{children:"some"})," of the appends. Most of the time, there is\r\nalready extra capacity, so you don't need to copy."]}),"\n",(0,r.jsxs)(n.p,{children:["To understand how this works, we need ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Amortized_analysis",children:(0,r.jsx)(n.strong,{children:"amortized\r\nanalysis"})}),". That shows us\r\nthat as long as we grow the array by a multiple of its current size, when we\r\naverage out the cost of a ",(0,r.jsx)(n.em,{children:"sequence"})," of appends, each append is ",(0,r.jsx)(n.em,{children:"O(1)"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We have our struct ready, so let's implement the functions to work with it. C\r\ndoesn't have constructors, so we declare a function to initialize a new chunk."}),"\n",(0,r.jsx)(n.p,{children:"^code init-chunk-h (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"And implement it thusly:"}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-c"}),"\n",(0,r.jsx)(n.p,{children:"The dynamic array starts off completely empty. We don't even allocate a raw\r\narray yet. To append a byte to the end of the chunk, we use a new function."}),"\n",(0,r.jsx)(n.p,{children:"^code write-chunk-h (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"This is where the interesting work happens."}),"\n",(0,r.jsx)(n.p,{children:"^code write-chunk"}),"\n",(0,r.jsxs)(n.p,{children:["The first thing we need to do is see if the current array already has capacity\r\nfor the new byte. If it doesn't, then we first need to grow the array to make\r\nroom. (We also hit this case on the very first write when the array is ",(0,r.jsx)(n.code,{children:"NULL"}),"\r\nand ",(0,r.jsx)(n.code,{children:"capacity"})," is 0.)"]}),"\n",(0,r.jsx)(n.p,{children:"To grow the array, first we figure out the new capacity and grow the array to\r\nthat size. Both of those lower-level memory operations are defined in a new\r\nmodule."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-c-include-memory (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"This is enough to get us started."}),"\n",(0,r.jsx)(n.p,{children:"^code memory-h"}),"\n",(0,r.jsxs)(n.p,{children:["This macro calculates a new capacity based on a given current capacity. In order\r\nto get the performance we want, the important part is that it ",(0,r.jsx)(n.em,{children:"scales"})," based on\r\nthe old size. We grow by a factor of two, which is pretty typical. 1.5\xd7 is\r\nanother common choice."]}),"\n",(0,r.jsxs)(n.p,{children:["We also handle when the current capacity is zero. In that case, we jump straight\r\nto eight elements instead of starting at one. That ",(0,r.jsx)(n.span,{name:"profile",children:"avoids"})," a little extra memory churn when the array is very\r\nsmall, at the expense of wasting a few bytes on very small chunks."]}),"\n",(0,r.jsxs)(n.aside,{name:"profile",children:["\n",(0,r.jsx)(n.p,{children:"I picked the number eight somewhat arbitrarily for the book. Most dynamic array\r\nimplementations have a minimum threshold like this. The right way to pick a\r\nvalue for this is to profile against real-world usage and see which constant\r\nmakes the best performance trade-off between extra grows versus wasted space."}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Once we know the desired capacity, we create or grow the array to that size\r\nusing ",(0,r.jsx)(n.code,{children:"GROW_ARRAY()"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"^code grow-array (2 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["This macro pretties up a function call to ",(0,r.jsx)(n.code,{children:"reallocate()"})," where the real work\r\nhappens. The macro itself takes care of getting the size of the array's element\r\ntype and casting the resulting ",(0,r.jsx)(n.code,{children:"void*"})," back to a pointer of the right type."]}),"\n",(0,r.jsxs)(n.p,{children:["This ",(0,r.jsx)(n.code,{children:"reallocate()"})," function is the single function we'll use for all dynamic\r\nmemory management in clox -- allocating memory, freeing it, and changing the\r\nsize of an existing allocation. Routing all of those operations through a single\r\nfunction will be important later when we add a garbage collector that needs to\r\nkeep track of how much memory is in use."]}),"\n",(0,r.jsxs)(n.p,{children:["The two size arguments passed to ",(0,r.jsx)(n.code,{children:"reallocate()"})," control which operation to\r\nperform:"]}),"\n",(0,r.jsxs)(n.table,{children:["\n  ",(0,r.jsxs)(n.thead,{children:["\n    ",(0,r.jsxs)(n.tr,{children:["\n      ",(0,r.jsx)(n.td,{children:"oldSize"}),"\n      ",(0,r.jsx)(n.td,{children:"newSize"}),"\n      ",(0,r.jsx)(n.td,{children:"Operation"}),"\n    "]}),"\n  "]}),"\n  ",(0,r.jsxs)(n.tbody,{children:["\n    ",(0,r.jsxs)(n.tr,{children:["\n      ",(0,r.jsx)(n.td,{children:"0"}),"\n      ",(0,r.jsx)(n.td,{children:"Non\u2011zero"}),"\n      ",(0,r.jsx)(n.td,{children:"Allocate new block."}),"\n    "]}),"\n    ",(0,r.jsxs)(n.tr,{children:["\n      ",(0,r.jsx)(n.td,{children:"Non\u2011zero"}),"\n      ",(0,r.jsx)(n.td,{children:"0"}),"\n      ",(0,r.jsx)(n.td,{children:"Free allocation."}),"\n    "]}),"\n    ",(0,r.jsxs)(n.tr,{children:["\n      ",(0,r.jsx)(n.td,{children:"Non\u2011zero"}),"\n      ",(0,r.jsxs)(n.td,{children:["Smaller\xa0than\xa0",(0,r.jsx)(n.code,{children:"oldSize"})]}),"\n      ",(0,r.jsx)(n.td,{children:"Shrink existing allocation."}),"\n    "]}),"\n    ",(0,r.jsxs)(n.tr,{children:["\n      ",(0,r.jsx)(n.td,{children:"Non\u2011zero"}),"\n      ",(0,r.jsxs)(n.td,{children:["Larger\xa0than\xa0",(0,r.jsx)(n.code,{children:"oldSize"})]}),"\n      ",(0,r.jsx)(n.td,{children:"Grow existing allocation."}),"\n    "]}),"\n  "]})]}),"\n",(0,r.jsx)(n.p,{children:"That sounds like a lot of cases to handle, but here's the implementation:"}),"\n",(0,r.jsx)(n.p,{children:"^code memory-c"}),"\n",(0,r.jsxs)(n.p,{children:["When ",(0,r.jsx)(n.code,{children:"newSize"})," is zero, we handle the deallocation case ourselves by calling\r\n",(0,r.jsx)(n.code,{children:"free()"}),". Otherwise, we rely on the C standard library's ",(0,r.jsx)(n.code,{children:"realloc()"})," function.\r\nThat function conveniently supports the other three aspects of our policy. When\r\n",(0,r.jsx)(n.code,{children:"oldSize"})," is zero, ",(0,r.jsx)(n.code,{children:"realloc()"})," is equivalent to calling ",(0,r.jsx)(n.code,{children:"malloc()"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The interesting cases are when both ",(0,r.jsx)(n.code,{children:"oldSize"})," and ",(0,r.jsx)(n.code,{children:"newSize"})," are not zero. Those\r\ntell ",(0,r.jsx)(n.code,{children:"realloc()"})," to resize the previously allocated block. If the new size is\r\nsmaller than the existing block of memory, it simply ",(0,r.jsx)(n.span,{name:"shrink",children:"updates"})," the size of the block and returns the same pointer\r\nyou gave it. If the new size is larger, it attempts to grow the existing block\r\nof memory."]}),"\n",(0,r.jsxs)(n.p,{children:["It can do that only if the memory after that block isn't already in use. If\r\nthere isn't room to grow the block, ",(0,r.jsx)(n.code,{children:"realloc()"})," instead allocates a ",(0,r.jsx)(n.em,{children:"new"})," block\r\nof memory of the desired size, copies over the old bytes, frees the old block,\r\nand then returns a pointer to the new block. Remember, that's exactly the\r\nbehavior we want for our dynamic array."]}),"\n",(0,r.jsxs)(n.p,{children:["Because computers are finite lumps of matter and not the perfect mathematical\r\nabstractions computer science theory would have us believe, allocation can fail\r\nif there isn't enough memory and ",(0,r.jsx)(n.code,{children:"realloc()"})," will return ",(0,r.jsx)(n.code,{children:"NULL"}),". We should\r\nhandle that."]}),"\n",(0,r.jsx)(n.p,{children:"^code out-of-memory (1 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["There's not really anything ",(0,r.jsx)(n.em,{children:"useful"})," that our VM can do if it can't get the\r\nmemory it needs, but we at least detect that and abort the process immediately\r\ninstead of returning a ",(0,r.jsx)(n.code,{children:"NULL"})," pointer and letting it go off the rails later."]}),"\n",(0,r.jsxs)(n.aside,{name:"shrink",children:["\n",(0,r.jsx)(n.p,{children:'Since all we passed in was a bare pointer to the first byte of memory, what does\r\nit mean to "update" the block\'s size? Under the hood, the memory allocator\r\nmaintains additional bookkeeping information for each block of heap-allocated\r\nmemory, including its size.'}),"\n",(0,r.jsxs)(n.p,{children:["Given a pointer to some previously allocated memory, it can find this\r\nbookkeeping information, which is necessary to be able to cleanly free it. It's\r\nthis size metadata that ",(0,r.jsx)(n.code,{children:"realloc()"})," updates."]}),"\n",(0,r.jsxs)(n.p,{children:["Many implementations of ",(0,r.jsx)(n.code,{children:"malloc()"})," store the allocated size in memory right\r\n",(0,r.jsx)(n.em,{children:"before"})," the returned address."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["OK, we can create new chunks and write instructions to them. Are we done? Nope!\r\nWe're in C now, remember, we have to manage memory ourselves, like in Ye Olden\r\nTimes, and that means ",(0,r.jsx)(n.em,{children:"freeing"})," it too."]}),"\n",(0,r.jsx)(n.p,{children:"^code free-chunk-h (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"The implementation is:"}),"\n",(0,r.jsx)(n.p,{children:"^code free-chunk"}),"\n",(0,r.jsxs)(n.p,{children:["We deallocate all of the memory and then call ",(0,r.jsx)(n.code,{children:"initChunk()"})," to zero out the\r\nfields leaving the chunk in a well-defined empty state. To free the memory, we\r\nadd one more macro."]}),"\n",(0,r.jsx)(n.p,{children:"^code free-array (3 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Like ",(0,r.jsx)(n.code,{children:"GROW_ARRAY()"}),", this is a wrapper around a call to ",(0,r.jsx)(n.code,{children:"reallocate()"}),". This one\r\nfrees the memory by passing in zero for the new size. I know, this is a lot of\r\nboring low-level stuff. Don't worry, we'll get a lot of use out of these in\r\nlater chapters and will get to program at a higher level. Before we can do that,\r\nthough, we gotta lay our own foundation."]}),"\n",(0,r.jsx)(n.h2,{id:"disassembling-chunks",children:"Disassembling Chunks"}),"\n",(0,r.jsx)(n.p,{children:"Now we have a little module for creating chunks of bytecode. Let's try it out by\r\nhand-building a sample chunk."}),"\n",(0,r.jsx)(n.p,{children:"^code main-chunk (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"Don't forget the include."}),"\n",(0,r.jsx)(n.p,{children:"^code main-include-chunk (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"Run that and give it a try. Did it work? Uh... who knows? All we've done is push\r\nsome bytes around in memory. We have no human-friendly way to see what's\r\nactually inside that chunk we made."}),"\n",(0,r.jsxs)(n.p,{children:["To fix this, we're going to create a ",(0,r.jsx)(n.strong,{children:"disassembler"}),". An ",(0,r.jsx)(n.strong,{children:"assembler"}),' is an\r\nold-school program that takes a file containing human-readable mnemonic names\r\nfor CPU instructions like "ADD" and "MULT" and translates them to their binary\r\nmachine code equivalent. A ',(0,r.jsx)(n.em,{children:"dis"}),"assembler goes in the other direction -- given a\r\nblob of machine code, it spits out a textual listing of the instructions."]}),"\n",(0,r.jsxs)(n.p,{children:["We'll implement something ",(0,r.jsx)(n.span,{name:"printer",children:"similar"}),". Given a chunk, it\r\nwill print out all of the instructions in it. A Lox ",(0,r.jsx)(n.em,{children:"user"})," won't use this, but\r\nwe Lox ",(0,r.jsx)(n.em,{children:"maintainers"})," will certainly benefit since it gives us a window into the\r\ninterpreter's internal representation of code."]}),"\n",(0,r.jsxs)(n.aside,{name:"printer",children:["\n",(0,r.jsxs)(n.p,{children:["In jlox, our analogous tool was the ",(0,r.jsx)(n.a,{href:"representing-code.html#a-not-very-pretty-printer",children:"AstPrinter class"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.code,{children:"main()"}),", after we create the chunk, we pass it to the disassembler."]}),"\n",(0,r.jsx)(n.p,{children:"^code main-disassemble-chunk (2 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Again, we whip up ",(0,r.jsx)(n.span,{name:"module",children:"yet another"})," module."]}),"\n",(0,r.jsxs)(n.aside,{name:"module",children:["\n",(0,r.jsx)(n.p,{children:"I promise you we won't be creating this many new files in later chapters."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"^code main-include-debug (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"Here's that header:"}),"\n",(0,r.jsx)(n.p,{children:"^code debug-h"}),"\n",(0,r.jsxs)(n.p,{children:["In ",(0,r.jsx)(n.code,{children:"main()"}),", we call ",(0,r.jsx)(n.code,{children:"disassembleChunk()"})," to disassemble all of the instructions\r\nin the entire chunk. That's implemented in terms of the other function, which\r\njust disassembles a single instruction. It shows up here in the header because\r\nwe'll call it from the VM in later chapters."]}),"\n",(0,r.jsx)(n.p,{children:"Here's a start at the implementation file:"}),"\n",(0,r.jsx)(n.p,{children:"^code debug-c"}),"\n",(0,r.jsxs)(n.p,{children:["To disassemble a chunk, we print a little header (so we can tell ",(0,r.jsx)(n.em,{children:"which"})," chunk\r\nwe're looking at) and then crank through the bytecode, disassembling each\r\ninstruction. The way we iterate through the code is a little odd. Instead of\r\nincrementing ",(0,r.jsx)(n.code,{children:"offset"})," in the loop, we let ",(0,r.jsx)(n.code,{children:"disassembleInstruction()"})," do it for\r\nus. When we call that function, after disassembling the instruction at the given\r\noffset, it returns the offset of the ",(0,r.jsx)(n.em,{children:"next"})," instruction. This is because, as\r\nwe'll see later, instructions can have different sizes."]}),"\n",(0,r.jsx)(n.p,{children:'The core of the "debug" module is this function:'}),"\n",(0,r.jsx)(n.p,{children:"^code disassemble-instruction"}),"\n",(0,r.jsx)(n.p,{children:"First, it prints the byte offset of the given instruction -- that tells us where\r\nin the chunk this instruction is. This will be a helpful signpost when we start\r\ndoing control flow and jumping around in the bytecode."}),"\n",(0,r.jsxs)(n.p,{children:["Next, it reads a single byte from the bytecode at the given offset. That's our\r\nopcode. We ",(0,r.jsx)(n.span,{name:"switch",children:"switch"})," on that. For each kind of\r\ninstruction, we dispatch to a little utility function for displaying it. On the\r\noff chance that the given byte doesn't look like an instruction at all -- a bug\r\nin our compiler -- we print that too. For the one instruction we do have,\r\n",(0,r.jsx)(n.code,{children:"OP_RETURN"}),", the display function is:"]}),"\n",(0,r.jsxs)(n.aside,{name:"switch",children:["\n",(0,r.jsx)(n.p,{children:"We have only one instruction right now, but this switch will grow throughout the\r\nrest of the book."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"^code simple-instruction"}),"\n",(0,r.jsx)(n.p,{children:"There isn't much to a return instruction, so all it does is print the name of\r\nthe opcode, then return the next byte offset past this instruction. Other\r\ninstructions will have more going on."}),"\n",(0,r.jsx)(n.p,{children:"If we run our nascent interpreter now, it actually prints something:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"== test chunk ==\r\n0000 OP_RETURN\n"})}),"\n",(0,r.jsx)(n.p,{children:'It worked! This is sort of the "Hello, world!" of our code representation. We\r\ncan create a chunk, write an instruction to it, and then extract that\r\ninstruction back out. Our encoding and decoding of the binary bytecode is\r\nworking.'}),"\n",(0,r.jsx)(n.h2,{id:"constants",children:"Constants"}),"\n",(0,r.jsxs)(n.p,{children:["Now that we have a rudimentary chunk structure working, let's start making it\r\nmore useful. We can store ",(0,r.jsx)(n.em,{children:"code"})," in chunks, but what about ",(0,r.jsx)(n.em,{children:"data"}),"? Many values\r\nthe interpreter works with are created at runtime as the result of operations."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-lox",children:"1 + 2;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The value 3 appears nowhere in the code here. However, the literals ",(0,r.jsx)(n.code,{children:"1"})," and ",(0,r.jsx)(n.code,{children:"2"}),'\r\ndo. To compile that statement to bytecode, we need some sort of instruction that\r\nmeans "produce a constant" and those literal values need to get stored in the\r\nchunk somewhere. In jlox, the Expr.Literal AST node held the value. We need a\r\ndifferent solution now that we don\'t have a syntax tree.']}),"\n",(0,r.jsx)(n.h3,{id:"representing-values",children:"Representing values"}),"\n",(0,r.jsxs)(n.p,{children:["We won't be ",(0,r.jsx)(n.em,{children:"running"})," any code in this chapter, but since constants have a foot\r\nin both the static and dynamic worlds of our interpreter, they force us to start\r\nthinking at least a little bit about how our VM should represent values."]}),"\n",(0,r.jsx)(n.p,{children:"For now, we're going to start as simple as possible -- we'll support only\r\ndouble-precision, floating-point numbers. This will obviously expand over time,\r\nso we'll set up a new module to give ourselves room to grow."}),"\n",(0,r.jsx)(n.p,{children:"^code value-h"}),"\n",(0,r.jsx)(n.p,{children:"This typedef abstracts how Lox values are concretely represented in C. That way,\r\nwe can change that representation without needing to go back and fix existing\r\ncode that passes around values."}),"\n",(0,r.jsxs)(n.p,{children:["Back to the question of where to store constants in a chunk. For small\r\nfixed-size values like integers, many instruction sets store the value directly\r\nin the code stream right after the opcode. These are called ",(0,r.jsx)(n.strong,{children:"immediate\r\ninstructions"})," because the bits for the value are immediately after the opcode."]}),"\n",(0,r.jsx)(n.p,{children:'That doesn\'t work well for large or variable-sized constants like strings. In a\r\nnative compiler to machine code, those bigger constants get stored in a separate\r\n"constant data" region in the binary executable. Then, the instruction to load a\r\nconstant has an address or offset pointing to where the value is stored in that\r\nsection.'}),"\n",(0,r.jsxs)(n.p,{children:["Most virtual machines do something similar. For example, the Java Virtual\r\nMachine ",(0,r.jsxs)(n.a,{href:"https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.4",children:["associates a ",(0,r.jsx)(n.strong,{children:"constant pool"})]})," with each compiled class.\r\nThat sounds good enough for clox to me. Each chunk will carry with it a list of\r\nthe values that appear as literals in the program. To keep things ",(0,r.jsx)(n.span,{name:"immediate",children:"simpler"}),", we'll put ",(0,r.jsx)(n.em,{children:"all"})," constants in there, even simple\r\nintegers."]}),"\n",(0,r.jsxs)(n.aside,{name:"immediate",children:["\n",(0,r.jsx)(n.p,{children:"In addition to needing two kinds of constant instructions -- one for immediate\r\nvalues and one for constants in the constant table -- immediates also force us\r\nto worry about alignment, padding, and endianness. Some architectures aren't\r\nhappy if you try to say, stuff a 4-byte integer at an odd address."}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"value-arrays",children:"Value arrays"}),"\n",(0,r.jsxs)(n.p,{children:["The constant pool is an array of values. The instruction to load a constant\r\nlooks up the value by index in that array. As with our ",(0,r.jsx)(n.span,{name:"generic",children:"bytecode"})," array, the compiler doesn't know how big the\r\narray needs to be ahead of time. So, again, we need a dynamic one. Since C\r\ndoesn't have generic data structures, we'll write another dynamic array data\r\nstructure, this time for Value."]}),"\n",(0,r.jsxs)(n.aside,{name:"generic",children:["\n",(0,r.jsx)(n.p,{children:"Defining a new struct and manipulation functions each time we need a dynamic\r\narray of a different type is a chore. We could cobble together some preprocessor\r\nmacros to fake generics, but that's overkill for clox. We won't need many more\r\nof these."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"^code value-array (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"As with the bytecode array in Chunk, this struct wraps a pointer to an array\r\nalong with its allocated capacity and the number of elements in use. We also\r\nneed the same three functions to work with value arrays."}),"\n",(0,r.jsx)(n.p,{children:"^code array-fns-h (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"The implementations will probably give you d\xe9j\xe0 vu. First, to create a new one:"}),"\n",(0,r.jsx)(n.p,{children:"^code value-c"}),"\n",(0,r.jsxs)(n.p,{children:["Once we have an initialized array, we can start ",(0,r.jsx)(n.span,{name:"add",children:"adding"}),"\r\nvalues to it."]}),"\n",(0,r.jsxs)(n.aside,{name:"add",children:["\n",(0,r.jsx)(n.p,{children:"Fortunately, we don't need other operations like insertion and removal."}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"^code write-value-array"}),"\n",(0,r.jsx)(n.p,{children:"The memory-management macros we wrote earlier do let us reuse some of the logic\r\nfrom the code array, so this isn't too bad. Finally, to release all memory used\r\nby the array:"}),"\n",(0,r.jsx)(n.p,{children:"^code free-value-array"}),"\n",(0,r.jsx)(n.p,{children:"Now that we have growable arrays of values, we can add one to Chunk to store the\r\nchunk's constants."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-constants (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"Don't forget the include."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-h-include-value (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"Ah, C, and its Stone Age modularity story. Where were we? Right. When we\r\ninitialize a new chunk, we initialize its constant list too."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-init-constant-array (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"Likewise, we free the constants when we free the chunk."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-free-constants (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"Next, we define a convenience method to add a new constant to the chunk. Our\r\nyet-to-be-written compiler could write to the constant array inside Chunk\r\ndirectly -- it's not like C has private fields or anything -- but it's a little\r\nnicer to add an explicit function."}),"\n",(0,r.jsx)(n.p,{children:"^code add-constant-h (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"Then we implement it."}),"\n",(0,r.jsx)(n.p,{children:"^code add-constant"}),"\n",(0,r.jsx)(n.p,{children:"After we add the constant, we return the index where the constant was appended\r\nso that we can locate that same constant later."}),"\n",(0,r.jsx)(n.h3,{id:"constant-instructions",children:"Constant instructions"}),"\n",(0,r.jsxs)(n.p,{children:["We can ",(0,r.jsx)(n.em,{children:"store"})," constants in chunks, but we also need to ",(0,r.jsx)(n.em,{children:"execute"})," them. In a\r\npiece of code like:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-lox",children:"print 1;\r\nprint 2;\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The compiled chunk needs to not only contain the values 1 and 2, but know ",(0,r.jsx)(n.em,{children:"when"}),"\r\nto produce them so that they are printed in the right order. Thus, we need an\r\ninstruction that produces a particular constant."]}),"\n",(0,r.jsx)(n.p,{children:"^code op-constant (1 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["When the VM executes a constant instruction, it ",(0,r.jsx)(n.span,{name:"load",children:'"loads"'}),"\r\nthe constant for use. This new instruction is a little more complex than\r\n",(0,r.jsx)(n.code,{children:"OP_RETURN"}),". In the above example, we load two different constants. A single\r\nbare opcode isn't enough to know ",(0,r.jsx)(n.em,{children:"which"})," constant to load."]}),"\n",(0,r.jsxs)(n.aside,{name:"load",children:["\n",(0,r.jsxs)(n.p,{children:['I\'m being vague about what it means to "load" or "produce" a constant because we\r\nhaven\'t learned how the virtual machine actually executes code at runtime yet.\r\nFor that, you\'ll have to wait until you get to (or skip ahead to, I suppose) the\r\n',(0,r.jsx)(n.a,{href:"a-virtual-machine.html",children:"next chapter"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["To handle cases like this, our bytecode -- like most others -- allows\r\ninstructions to have ",(0,r.jsx)(n.span,{name:"operand",children:(0,r.jsx)(n.strong,{children:"operands"})}),". These are stored\r\nas binary data immediately after the opcode in the instruction stream and let us\r\nparameterize what the instruction does."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"image/chunks-of-bytecode/format.png",alt:"OP_CONSTANT is a byte for\nthe opcode followed by a byte for the constant index."})}),"\n",(0,r.jsxs)(n.p,{children:['Each opcode determines how many operand bytes it has and what they mean. For\r\nexample, a simple operation like "return" may have no operands, where an\r\ninstruction for "load local variable" needs an operand to identify which\r\nvariable to load. Each time we add a new opcode to clox, we specify what its\r\noperands look like -- its ',(0,r.jsx)(n.strong,{children:"instruction format"}),"."]}),"\n",(0,r.jsxs)(n.aside,{name:"operand",children:["\n",(0,r.jsxs)(n.p,{children:["Bytecode instruction operands are ",(0,r.jsx)(n.em,{children:"not"})," the same as the operands passed to an\r\narithmetic operator. You'll see when we get to expressions that arithmetic\r\noperand values are tracked separately. Instruction operands are a lower-level\r\nnotion that modify how the bytecode instruction itself behaves."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["In this case, ",(0,r.jsx)(n.code,{children:"OP_CONSTANT"})," takes a single byte operand that specifies which\r\nconstant to load from the chunk's constant array. Since we don't have a compiler\r\nyet, we \"hand-compile\" an instruction in our test chunk."]}),"\n",(0,r.jsx)(n.p,{children:"^code main-constant (1 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["We add the constant value itself to the chunk's constant pool. That returns the\r\nindex of the constant in the array. Then we write the constant instruction,\r\nstarting with its opcode. After that, we write the one-byte constant index\r\noperand. Note that ",(0,r.jsx)(n.code,{children:"writeChunk()"})," can write opcodes or operands. It's all raw\r\nbytes as far as that function is concerned."]}),"\n",(0,r.jsx)(n.p,{children:"If we try to run this now, the disassembler is going to yell at us because it\r\ndoesn't know how to decode the new instruction. Let's fix that."}),"\n",(0,r.jsx)(n.p,{children:"^code disassemble-constant (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"This instruction has a different instruction format, so we write a new helper\r\nfunction to disassemble it."}),"\n",(0,r.jsx)(n.p,{children:"^code constant-instruction"}),"\n",(0,r.jsxs)(n.p,{children:["There's more going on here. As with ",(0,r.jsx)(n.code,{children:"OP_RETURN"}),", we print out the name of the\r\nopcode. Then we pull out the constant index from the subsequent byte in the\r\nchunk. We print that index, but that isn't super useful to us human readers. So\r\nwe also look up the actual constant value -- since constants ",(0,r.jsx)(n.em,{children:"are"})," known at\r\ncompile time after all -- and display the value itself too."]}),"\n",(0,r.jsx)(n.p,{children:'This requires some way to print a clox Value. That function will live in the\r\n"value" module, so we include that.'}),"\n",(0,r.jsx)(n.p,{children:"^code debug-include-value (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"Over in that header, we declare:"}),"\n",(0,r.jsx)(n.p,{children:"^code print-value-h (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"And here's an implementation:"}),"\n",(0,r.jsx)(n.p,{children:"^code print-value"}),"\n",(0,r.jsx)(n.p,{children:"Magnificent, right? As you can imagine, this is going to get more complex once\r\nwe add dynamic typing to Lox and have values of different types."}),"\n",(0,r.jsxs)(n.p,{children:["Back in ",(0,r.jsx)(n.code,{children:"constantInstruction()"}),", the only remaining piece is the return value."]}),"\n",(0,r.jsx)(n.p,{children:"^code return-after-operand (1 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Remember that ",(0,r.jsx)(n.code,{children:"disassembleInstruction()"})," also returns a number to tell the\r\ncaller the offset of the beginning of the ",(0,r.jsx)(n.em,{children:"next"})," instruction. Where ",(0,r.jsx)(n.code,{children:"OP_RETURN"}),"\r\nwas only a single byte, ",(0,r.jsx)(n.code,{children:"OP_CONSTANT"})," is two -- one for the opcode and one for\r\nthe operand."]}),"\n",(0,r.jsx)(n.h2,{id:"line-information",children:"Line Information"}),"\n",(0,r.jsx)(n.p,{children:"Chunks contain almost all of the information that the runtime needs from the\r\nuser's source code. It's kind of crazy to think that we can reduce all of the\r\ndifferent AST classes that we created in jlox down to an array of bytes and an\r\narray of constants. There's only one piece of data we're missing. We need it,\r\neven though the user hopes to never see it."}),"\n",(0,r.jsx)(n.p,{children:"When a runtime error occurs, we show the user the line number of the offending\r\nsource code. In jlox, those numbers live in tokens, which we in turn store in\r\nthe AST nodes. We need a different solution for clox now that we've ditched\r\nsyntax trees in favor of bytecode. Given any bytecode instruction, we need to be\r\nable to determine the line of the user's source program that it was compiled\r\nfrom."}),"\n",(0,r.jsxs)(n.p,{children:["There are a lot of clever ways we could encode this. I took the absolute ",(0,r.jsx)(n.span,{name:"side",children:"simplest"})," approach I could come up with, even though it's\r\nembarrassingly inefficient with memory. In the chunk, we store a separate array\r\nof integers that parallels the bytecode. Each number in the array is the line\r\nnumber for the corresponding byte in the bytecode. When a runtime error occurs,\r\nwe look up the line number at the same index as the current instruction's offset\r\nin the code array."]}),"\n",(0,r.jsxs)(n.aside,{name:"side",children:["\n",(0,r.jsxs)(n.p,{children:["This braindead encoding does do one thing right: it keeps the line information\r\nin a ",(0,r.jsx)(n.em,{children:"separate"})," array instead of interleaving it in the bytecode itself. Since\r\nline information is only used when a runtime error occurs, we don't want it\r\nbetween the instructions, taking up precious space in the CPU cache and causing\r\nmore cache misses as the interpreter skips past it to get to the opcodes and\r\noperands it cares about."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"To implement this, we add another array to Chunk."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-lines (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"Since it exactly parallels the bytecode array, we don't need a separate count or\r\ncapacity. Every time we touch the code array, we make a corresponding change to\r\nthe line number array, starting with initialization."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-null-lines (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"And likewise deallocation:"}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-free-lines (1 before, 1 after)"}),"\n",(0,r.jsxs)(n.p,{children:["When we write a byte of code to the chunk, we need to know what source line it\r\ncame from, so we add an extra parameter in the declaration of ",(0,r.jsx)(n.code,{children:"writeChunk()"}),"."]}),"\n",(0,r.jsx)(n.p,{children:"^code write-chunk-with-line-h (1 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"And in the implementation:"}),"\n",(0,r.jsx)(n.p,{children:"^code write-chunk-with-line (1 after)"}),"\n",(0,r.jsx)(n.p,{children:"When we allocate or grow the code array, we do the same for the line info too."}),"\n",(0,r.jsx)(n.p,{children:"^code write-chunk-line (2 before, 1 after)"}),"\n",(0,r.jsx)(n.p,{children:"Finally, we store the line number in the array."}),"\n",(0,r.jsx)(n.p,{children:"^code chunk-write-line (1 before, 1 after)"}),"\n",(0,r.jsx)(n.h3,{id:"disassembling-line-information",children:"Disassembling line information"}),"\n",(0,r.jsxs)(n.p,{children:["Alright, let's try this out with our little, uh, artisanal chunk. First, since\r\nwe added a new parameter to ",(0,r.jsx)(n.code,{children:"writeChunk()"}),", we need to fix those calls to pass\r\nin some -- arbitrary at this point -- line number."]}),"\n",(0,r.jsx)(n.p,{children:"^code main-chunk-line (1 before, 2 after)"}),"\n",(0,r.jsx)(n.p,{children:"Once we have a real front end, of course, the compiler will track the current\r\nline as it parses and pass that in."}),"\n",(0,r.jsx)(n.p,{children:"Now that we have line information for every instruction, let's put it to good\r\nuse. In our disassembler, it's helpful to show which source line each\r\ninstruction was compiled from. That gives us a way to map back to the original\r\ncode when we're trying to figure out what some blob of bytecode is supposed to\r\ndo. After printing the offset of the instruction -- the number of bytes from the\r\nbeginning of the chunk -- we show its source line."}),"\n",(0,r.jsx)(n.p,{children:"^code show-location (2 before, 2 after)"}),"\n",(0,r.jsxs)(n.p,{children:["Bytecode instructions tend to be pretty fine-grained. A single line of source\r\ncode often compiles to a whole sequence of instructions. To make that more\r\nvisually clear, we show a ",(0,r.jsx)(n.code,{children:"|"})," for any instruction that comes from the same\r\nsource line as the preceding one. The resulting output for our handwritten\r\nchunk looks like:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-text",children:"== test chunk ==\r\n0000  123 OP_CONSTANT         0 '1.2'\r\n0002    | OP_RETURN\n"})}),"\n",(0,r.jsxs)(n.p,{children:["We have a three-byte chunk. The first two bytes are a constant instruction that\r\nloads 1.2 from the chunk's constant pool. The first byte is the ",(0,r.jsx)(n.code,{children:"OP_CONSTANT"}),"\r\nopcode and the second is the index in the constant pool. The third byte (at\r\noffset 2) is a single-byte return instruction."]}),"\n",(0,r.jsx)(n.p,{children:"In the remaining chapters, we will flesh this out with lots more kinds of\r\ninstructions. But the basic structure is here, and we have everything we need\r\nnow to completely represent an executable piece of code at runtime in our\r\nvirtual machine. Remember that whole family of AST classes we defined in jlox?\r\nIn clox, we've reduced that down to three arrays: bytes of code, constant\r\nvalues, and line information for debugging."}),"\n",(0,r.jsxs)(n.p,{children:["This reduction is a key reason why our new interpreter will be faster than jlox.\r\nYou can think of bytecode as a sort of compact serialization of the AST, highly\r\noptimized for how the interpreter will deserialize it in the order it needs as\r\nit executes. In the ",(0,r.jsx)(n.a,{href:"a-virtual-machine.html",children:"next chapter"}),", we will see how the virtual machine does\r\nexactly that."]}),"\n",(0,r.jsxs)(n.div,{className:"challenges",children:["\n",(0,r.jsx)(n.h2,{id:"challenges",children:"Challenges"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Our encoding of line information is hilariously wasteful of memory. Given\r\nthat a series of instructions often correspond to the same source line, a\r\nnatural solution is something akin to ",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Run-length_encoding",children:"run-length encoding"})," of the line\r\nnumbers."]}),"\n",(0,r.jsxs)(n.p,{children:["Devise an encoding that compresses the line information for a\r\nseries of instructions on the same line. Change ",(0,r.jsx)(n.code,{children:"writeChunk()"})," to write this\r\ncompressed form, and implement a ",(0,r.jsx)(n.code,{children:"getLine()"})," function that, given the index\r\nof an instruction, determines the line where the instruction occurs."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Hint: It's not necessary for ",(0,r.jsx)(n.code,{children:"getLine()"})," to be particularly efficient.\r\nSince it is called only when a runtime error occurs, it is well off the\r\ncritical path where performance matters."]})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Because ",(0,r.jsx)(n.code,{children:"OP_CONSTANT"})," uses only a single byte for its operand, a chunk may\r\nonly contain up to 256 different constants. That's small enough that people\r\nwriting real-world code will hit that limit. We could use two or more bytes\r\nto store the operand, but that makes ",(0,r.jsx)(n.em,{children:"every"})," constant instruction take up\r\nmore space. Most chunks won't need that many unique constants, so that\r\nwastes space and sacrifices some locality in the common case to support the\r\nrare case."]}),"\n",(0,r.jsxs)(n.p,{children:["To balance those two competing aims, many instruction sets feature multiple\r\ninstructions that perform the same operation but with operands of different\r\nsizes. Leave our existing one-byte ",(0,r.jsx)(n.code,{children:"OP_CONSTANT"})," instruction alone, and\r\ndefine a second ",(0,r.jsx)(n.code,{children:"OP_CONSTANT_LONG"})," instruction. It stores the operand as a\r\n24-bit number, which should be plenty."]}),"\n",(0,r.jsx)(n.p,{children:"Implement this function:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-c",children:"void writeConstant(Chunk* chunk, Value value, int line) {\r\n  // Implement me...\r\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:["It adds ",(0,r.jsx)(n.code,{children:"value"})," to ",(0,r.jsx)(n.code,{children:"chunk"}),"'s constant array and then writes an appropriate\r\ninstruction to load the constant. Also add support to the disassembler for\r\n",(0,r.jsx)(n.code,{children:"OP_CONSTANT_LONG"})," instructions."]}),"\n",(0,r.jsx)(n.p,{children:"Defining two instructions seems to be the best of both worlds. What\r\nsacrifices, if any, does it force on us?"}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["Our ",(0,r.jsx)(n.code,{children:"reallocate()"})," function relies on the C standard library for dynamic\r\nmemory allocation and freeing. ",(0,r.jsx)(n.code,{children:"malloc()"})," and ",(0,r.jsx)(n.code,{children:"free()"})," aren't magic. Find\r\na couple of open source implementations of them and explain how they work.\r\nHow do they keep track of which bytes are allocated and which are free?\r\nWhat is required to allocate a block of memory? Free it? How do they make\r\nthat efficient? What do they do about fragmentation?"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.em,{children:"Hardcore mode:"})," Implement ",(0,r.jsx)(n.code,{children:"reallocate()"})," without calling ",(0,r.jsx)(n.code,{children:"realloc()"}),",\r\n",(0,r.jsx)(n.code,{children:"malloc()"}),", or ",(0,r.jsx)(n.code,{children:"free()"}),". You are allowed to call ",(0,r.jsx)(n.code,{children:"malloc()"})," ",(0,r.jsx)(n.em,{children:"once"}),", at the\r\nbeginning of the interpreter's execution, to allocate a single big block of\r\nmemory, which your ",(0,r.jsx)(n.code,{children:"reallocate()"})," function has access to. It parcels out\r\nblobs of memory from that single region, your own personal heap. It's your\r\njob to define how it does that."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.div,{className:"design-note",children:["\n",(0,r.jsx)(n.h2,{id:"design-note-test-your-language",children:"Design Note: Test Your Language"}),"\n",(0,r.jsxs)(n.p,{children:["We're almost halfway through the book and one thing we haven't talked about is\r\n",(0,r.jsx)(n.em,{children:"testing"})," your language implementation. That's not because testing isn't\r\nimportant. I can't possibly stress enough how vital it is to have a good,\r\ncomprehensive test suite for your language."]}),"\n",(0,r.jsxs)(n.p,{children:["I wrote a ",(0,r.jsx)(n.a,{href:"https://github.com/munificent/craftinginterpreters/tree/master/test",children:"test suite for Lox"})," (which you are welcome to use on your own\r\nLox implementation) before I wrote a single word of this book. Those tests found\r\ncountless bugs in my implementations."]}),"\n",(0,r.jsx)(n.p,{children:"Tests are important in all software, but they're even more important for a\r\nprogramming language for at least a couple of reasons:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Users expect their programming languages to be rock solid."}),' We are so\r\nused to mature, stable compilers and interpreters that "It\'s your code, not\r\nthe compiler" is ',(0,r.jsx)(n.a,{href:"https://blog.codinghorror.com/the-first-rule-of-programming-its-always-your-fault/",children:"an ingrained part of software culture"}),". If there\r\nare bugs in your language implementation, users will go through the full\r\nfive stages of grief before they can figure out what's going on, and you\r\ndon't want to put them through all that."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"A language implementation is a deeply interconnected piece of software."}),"\r\nSome codebases are broad and shallow. If the file loading code is broken in\r\nyour text editor, it -- hopefully! -- won't cause failures in the text\r\nrendering on screen. Language implementations are narrower and deeper,\r\nespecially the core of the interpreter that handles the language's actual\r\nsemantics. That makes it easy for subtle bugs to creep in caused by weird\r\ninteractions between various parts of the system. It takes good tests to\r\nflush those out."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"The input to a language implementation is, by design, combinatorial."}),"\r\nThere are an infinite number of possible programs a user could write, and\r\nyour implementation needs to run them all correctly. You obviously can't\r\ntest that exhaustively, but you need to work hard to cover as much of the\r\ninput space as you can."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Language implementations are often complex, constantly changing, and full\r\nof optimizations."})," That leads to gnarly code with lots of dark corners\r\nwhere bugs can hide."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["All of that means you're gonna want a lot of tests. But ",(0,r.jsx)(n.em,{children:"what"})," tests? Projects\r\nI've seen focus mostly on end-to-end \"language tests\". Each test is a program\r\nwritten in the language along with the output or errors it is expected to\r\nproduce. Then you have a test runner that pushes the test program through your\r\nlanguage implementation and validates that it does what it's supposed to.\r\nWriting your tests in the language itself has a few nice advantages:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"The tests aren't coupled to any particular API or internal architecture\r\ndecisions of the implementation. This frees you to reorganize or rewrite\r\nparts of your interpreter or compiler without needing to update a slew of\r\ntests."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"You can use the same tests for multiple implementations of the language."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"Tests can often be terse and easy to read and maintain since they are\r\nsimply scripts in your language."}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"It's not all rosy, though:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["End-to-end tests help you determine ",(0,r.jsx)(n.em,{children:"if"})," there is a bug, but not ",(0,r.jsx)(n.em,{children:"where"})," the\r\nbug is. It can be harder to figure out where the erroneous code in the\r\nimplementation is because all the test tells you is that the right output\r\ndidn't appear."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:"It can be a chore to craft a valid program that tickles some obscure corner\r\nof the implementation. This is particularly true for highly optimized\r\ncompilers where you may need to write convoluted code to ensure that you\r\nend up on just the right optimization path where a bug may be hiding."}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:["The overhead can be high to fire up the interpreter, parse, compile, and\r\nrun each test script. With a big suite of tests -- which you ",(0,r.jsx)(n.em,{children:"do"})," want,\r\nremember -- that can mean a lot of time spent waiting for the tests to\r\nfinish running."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["I could go on, but I don't want this to turn into a sermon. Also, I don't\r\npretend to be an expert on ",(0,r.jsx)(n.em,{children:"how"})," to test languages. I just want you to\r\ninternalize how important it is ",(0,r.jsx)(n.em,{children:"that"})," you test yours. Seriously. Test your\r\nlanguage. You'll thank me for it."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const i={},o=r.createContext(i);function s(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);