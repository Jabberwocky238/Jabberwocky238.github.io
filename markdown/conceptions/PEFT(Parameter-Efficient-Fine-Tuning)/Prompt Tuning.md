指示微调（Prompt-tuning）：

• 在此方法中，**微调的目标是在模型输入之前添加一个可学习的“提示”（prompt）**，这个提示通常是一个或一系列向量，它们代表了特定任务的上下文信息。

• 提示可以是文本形式，即直接插入到输入序列中的词汇项（tokens），也可以是对嵌入层权重的更新，这些权重用于生成每个任务特有的前缀部分。

• 通过优化这些提示向量，模型可以在不改变主体模型参数的情况下，根据不同的任务调整其输出。

![[Pasted image 20240412111322.png]]

现在，上面说明的这个概念被称为硬提示调整（hard prompt tuning），因为我们直接更改不可微分的离散输入标记。与_硬_提示调整相反，软提示调整（soft prompt tuning）将输入标记的嵌入与可训练张量连接起来，该张量可以通过反向传播进行优化，以提高目标任务的建模性能。

软提示不同于离散文本提示，因为它们是通过反向传播（back-propagation）获得的，因此根据来自标记数据集的损失反馈进行调整。软提示调整（Soft prompt tuning） 的参数效率明显高于 full-finetuning，尽管 soft prompt-finetuned 模型的建模性能可能略差，如下图所示。

![[Pasted image 20240412111937.png]]

[[Prefix Tuning#从提示调优（Prompt Tuning）到前缀调优（Prefix Tuning）]]