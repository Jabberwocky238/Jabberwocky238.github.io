集成学习算法从集成思想的架构分为：Bagging 和 Boosting

# Voting

Voting是一种集成学习，它将来自多个机器学习模型的预测结合起来产生结果。在整个数据集上训练多个基础模型来进行预测。每个模型预测被认为是一个“投票”。得到多数选票的预测将被选为最终预测。

有两种类型的投票用于汇总基础预测-硬投票和软投票。

硬投票选择投票数最高的预测作为最终预测，而软投票将每个模型中每个类的概率结合起来，选择概率最高的类作为最终预测。

在回归问题中，它的工作方式有些不同，因为我们不是寻找频率最高的类，而是采用每个模型的预测并计算它们的平均值，从而得出最终的预测。

# Bagging算法
(Bootstrapped Aggregation)：装袋算法
![[Pasted image 20240414140951.png]]
- 没有一个模型是在整个数据集上训练的
- 如果原始数据集部分元素发生异常变化，可能会影响子模型，而不影响聚合后的模型，
- **因此：“取偏差小 ↓ 、方差大 ↑ 的基础模型**，将其聚合，得到偏差小、方差小的模型”
- 
### random forest

- 训练时间复杂度：O(n * $lg_{n}$ * d * k)；每个基础模型都可在计算机的不同核上，进行训练
- 运行时间复杂度：O(depth * k)，低延时
- 运行时的空间复杂度：O(Decision Trees * k)

### 极度随机树(Extremely randomized trees)

常规的决策树中，分裂数值特征，是将特征值进行排序，将每个值进行信息增益计算，然后选择信息增益最大的阈值。而极度随机树在分裂数值特征时，只随机选取**一个或一个数值子集**进行计算信息增益，比较选取最大信息增益的阈值。
随机森林：列抽样+行抽样+决策树+聚合
极度随机树：列抽样+行抽样+**分裂数值特征时的数值随机化**+决策树+聚合
小结：极度随机树再次增加随机化，有利减少方差，可能会稍微增加偏差

# Boosting算法
提升算法，可将弱学习器提升为强学习器的算法
**在Bagging中**：基础模型为 低偏差 ↓ ，高方差 ↑ ；如深度特别深的 决策树
**在Boosting中**：基础模型为 高偏差 ↑ ，低方差 ↓ ；如深度特别浅的 决策树(如：深度为2，3)
每个阶段的每个基础模型，都被训练来拟合前一阶段最终结束时的残差
#### 残差(Residuals)、损失函数(Loss functions)和梯度(Residuals)
![[Pasted image 20240414142156.png]]

### 梯度提升(Gradient Boosting)
当基础模型 为决策树时，那么这样的集成模型，常被称为**GBDT(梯度提升树)**


### Adaboost
AdaBoost算法，常用于计算机视觉的人脸检测，同时错误分类的点会得到更多的权重(比如上采样增加错误分类的点数)，然后进行多次基础模型的学习，最终组合模型
![[Pasted image 20240414142507.png]]

# Stacking
堆叠法
1. 使用多个基础模型(KNN、朴素贝叶斯、决策树、SVM、逻辑回归等)，独立在训练集上学习各模型（此处的基础模型，可以有较好的方差与偏差平衡，同时基础模型越不同，组合后的效果越好）
2. 将各个基础模型预测的结果构建一个新的数据集
3. 将构建好的新数据集，进行最终的模型学习(最终的一个模型，可以为任何模型，也被Meta-Classifier(元分类器)
4. Bagging VS Stacking ，前者最终投票聚合，后者最终训练一个模型
5. 堆叠法在数据量特别大时(比如样本容量为百万)，效果可能才会更有意义

# Cascading
级联法
级联分类器又称串联分类算法，一般用于错误分类成本较大的情况，比如反欺诈模型；在以下案例，以标签1为欺诈，0为正常

![[Pasted image 20240414142805.png]]

![[Pasted image 20240414142838.png]]

- 模型越往后越复杂
- 当所有模型都无法确认，后面就直接人工干预处理
- 高概率完美分类后的数据点，都不会显示给下一个模型

![[Pasted image 20240414142910.png]]

